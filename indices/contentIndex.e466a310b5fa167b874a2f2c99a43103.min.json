{"/":{"title":"xeome.github.io","content":"\n### About me\n\nHi, I'm a computer science student in my first year of university. I work with Linux and Unix-like systems. I enjoy optimizing systems to get the most out of hardware. I'm working on my own Linux distribution and also contribute to CachyOS, which is a similar distribution to mine. I also write Linux internals documentation. I've recently started learning eBPF and XDP.\n\nGithub: \u003chttps://github.com/xeome\u003e\n\n### Highlights\n\n#### CachyOS\n\nArch Linux based distribution with heavy optimizations and multi architecture support for ultimate desktop experience.\n\nLink: \u003chttps://github.com/CachyOS\u003e\n\n#### JomOS\n\nJomOS is a meta Linux distribution which allows users to mix-and-match well tested configurations and optimizations with little to no effort. JomOS integrates these configurations into one largely cohesive system.\n\nLink: \u003chttps://github.com/xeome/jomOS\u003e\n\nDocument: [[notes/JomOS]]\n\n#### xeome.github.io\n\nThis site contains all the documents i write about Linux and other topics.\n\nTo access all my notes, click here \u003chttps://xeome.github.io/notes/\u003e.\n","lastmodified":"2022-09-25T20:18:24.415747785Z","tags":null},"/notes/Btrfs-Maintenance":{"title":"Btrfs Maintenance","content":"\nLinks: [[notes/Linux]], [[notes/Post install optimizations]]\n\n# Btrfs Maintenance\n\n## Btrfs scrub\n\nScrubbing reads all data and metadata from devices and verifies checksums. It is not mandatory, but it may detect problems with faulty hardware early because it touches data that may not be in use and causes bit rot.\n\nIf there is data/metadata redundancy, such as DUP or RAID1/5/6 profiles, scrub can automatically repair the data if a good copy is available.\n\nYou can use `sudo btrfs scrub start /` to start the scan and `sudo btrfs scrub status /` to check the status of it.\n\n## Btrfs balance\n\nThe balance command can do a lot of things, but it primarily moves data in large chunks. It is used here to reclaim the space of the underutilized chunks so that it can be allocated again based on current needs.\n\nThe goal is to avoid situations in which it is impossible to allocate new metadata chunks, for example, because the entire device space is reserved for all the chunks, even though the total space occupied is smaller and the allocation should succeed.\n\nThe balance operation requires enough space to shuffle data around. By workspace, we mean device space with no filesystem chunks on it, not free space as reported by df, for example.\n\nThe balance command may fail due to a lack of space, but this is considered a minor error because the internal filesystem layout may prevent the command from finding enough workspace. This could be a good time to inspect the space manually.\n\nRunning `btrfs balance start` without any filters, would re-write every Data and Metadata chunk on the disk. Usually, this is not what we want. Instead use the `usage` filter to limit what blocks should be balanced.\n\nUsing `-dusage=5` we limit balance to compact data blocks that are less than 5% full. This is a good start, and we can increase it to 10-15% or more if needed. A small (less than 100GiB) filesystem may need a higher number.\n\n`sudo btrfs balance start --bg -dusage=5 /path` to start the balance\n`sudo btrfs balance status /path` to check status\n\n**Expected outcome:** If all underutilized chunks are removed, the total value in the output of `btrfs fi df /path` should be lower than before. Examine the logs.\n\n## trimming\n\nAlthough not specifically related to btrfs, this still needs to be mentioned.\n\nThe TRIM (aka discard) operation can instruct the underlying device to optimize blocks that are not being used by the filesystem. The fstrim utility performs this task on demand.\n\nThis makes sense for SSDs or other types of storage that can translate TRIM actions into useful data (eg. thin-provisioned storage).\n\nYou can use `sudo fstrim --fstab --verbose` to run fstrim on all mounted filesystems mentioned in /etc/fstab on devices that support the discard operation.\n\n`--fstab` parameter documentation:\nOn devices that support the discard operation, trim all mounted filesystems listed in /etc/fstab. If the root filesystem is missing from the file, it is determined from the kernel command line. Other provided options, such as ---offset, --length, and --minimum, are applied to all of these devices. Errors originating from filesystems that do not support the discard operation, as well as read-only devices, autofs, and read-only filesystems, are silently ignored. Filesystems with the mount option \"X-fstrim.notrim\" are skipped.\n\n`--verbose` parameter documentation:\n\nVerbose execution. With this option fstrim will output the number of bytes passed from the filesystem down the block stack to the device for potential discard. This number is a maximum discard amount from the storage device’s perspective, because FITRIM ioctl called repeated will keep sending the same sectors for discard repeatedly.\n\n## Sources\n\n\u003chttps://github.com/kdave/btrfsmaintenance\u003e\n\n\u003chttps://man.archlinux.org/man/fstrim.8.en\u003e\n","lastmodified":"2022-09-25T20:18:24.415747785Z","tags":null},"/notes/Emulating-Cortex-A72":{"title":"Emulating Cortex A72","content":"\nLinks: [[notes/Linux]]\n\n# Starting out (Preparing for emulation)\n\n- Create a Project directory.\n\n```sh\n$ mkdir rpi_image\n$ cd rpi_image\n```\n\n- Download and decompress the Debian RasPi4 image.\n\n```sh\n$ wget https://raspi.debian.net/tested/20220808_raspi_4_bookworm.img.xz\n$ xz --decompress 20220808_raspi_4_bookworm.img.xz\n```\n\n- Using `fdisk`, determine the starting sector number.\n\n```sh\n$ fdisk -l 20220808_raspi_4_bookworm.img\n```\n\n```\nDisk 20220808_raspi_4_bookworm.img: 1,95 GiB, 2097152000 bytes, 4096000 sectors\nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisklabel type: dos\nDisk identifier: 0xf7e489e2\n\nDevice         Boot  Start     End Sectors  Size Id Type\n20220808_raspi_4_bookworm.img1        8192  819199  811008  396M  c W95 FAT32\n20220808_raspi_4_bookworm.img2      819200 4095999 3276800  1,6G 83 Linux\n```\n\n- Before we mount the image to do some stuff, we need to get an offset in order to correctly mount.\u003cbr\u003eFind the `Start` number in the second partition `$something.img2`. It's `819200` in my case. Multiply it by 512, which equals `419430400` in my case.\n\n- Create a mount directory:\n\n```sh\n$ mkdir /mnt/raspi4\n```\n\n- We can now mount image:\n\n```sh\n$ sudo mount -o offset=419430400 20220808_raspi_4_bookworm.img /mnt/raspi4\n```\n\n- Now we can extract kernel and initrd from image:\u003cbr\u003e(NOTE: We are cd'd into rpi_image directory)\n\n```sh\n$ cp /mnt/raspi4/vmlinuz .\n$ cp /mnt/raspi4/initrd.img .\n```\n\n- Finally, we need to edit `fstab` for slicker(?) mounting via QEMU\n\n```\n$ nano /mnt/raspi4/etc/fstab\n```\n\n```sh\n# The root file system has fs_passno=1 as per fstab(5) for automatic fsck.\nLABEL=RASPIROOT / ext4 rw 0 1\n# All other file systems have fs_passno=2 as per fstab(5) for automatic fsck.\nLABEL=RASPIFIRM /boot/firmware vfat rw 0 2\n```\n\n- Replace `LABEL=RASPIROOT` with `/dev/vda2`\n\n- Replace `LABEL=RASPIFIRM` with `/dev/vda1`\n\n- The file should look something like this.\n\n```sh\n# The root file system has fs_passno=1 as per fstab(5) for automatic fsck.\n/dev/vda2 / ext4 rw 0 1\n# All other file systems have fs_passno=2 as per fstab(5) for automatic fsck.\n/dev/vda1 /boot/firmware vfat rw 0 2\n```\n\n- We can now convert the image to qcow2.\n\n```sh\nqemu-img convert -f raw -O qcow2 20220808_raspi_4_bookworm.img rpi.qcow2\n```\n\n# Emulation Time!\n\n- We can finally start making our launch script.\n\n```sh\n$ nano rpistart.sh\n```\n\n**rpistart.sh**\n\n```sh\n#!/bin/bash\nscreen -mS raspberry-pi-4 \\\nsudo qemu-system-aarch64 \\\n-M virt \\\n-m 4096 -smp 4 \\\n-cpu cortex-a72 \\\n-kernel vmlinuz \\\n-initrd initrd.img \\\n-append \"root=/dev/vda2 panic=1 rootfstype=ext4 rw\" \\\n-hda rpi.qcow2 \\\n-no-reboot \\\n-nographic\n```\n\n- Paste the above into the script and save.\n\n### Some info about script\n\n- Since QEMU doesn't natively support Raspberry Pi 4(b), our only option is to virtualize Cortex A72 (Which is CPU used in Raspberry Pi 4(b)).\n- `-nographic` because _who needs graphics._\n- `screen` is used 'cuz _why not_.\n\n- Make script executable\n\n```sh\n$ chmod +x rpistart.sh\n```\n\n- __And you should_ be able to run QEMU instance._*\n- LAUNCH!\n\n```sh\n$ ./rpistart.sh\n```\n\n# Some wacky reality.\n\nYou have booted into nice Debian. Oh, btw, username is passwordless `root`.\u003cbr\u003eBut there are two problems:\n\n- Not enough space!\n- No internet!\n\n## Not enough space!\n\nVery simple. Just:\n\n- poweroff the VM.\n\n```sh\nVM$ poweroff\n```\n\n- In our `rpi_image` directory, we can resize `qcow2` image via:\n\n```sh\n$ qemu-img resize rpi.qcow2 +4G\n```\n\n```sh\n# Side note: You can also set exact size by getting rid of that + sign.\n```\n\n- Now we need to boot into VM once again:\n\n```sh\n$ ./rpistart.sh\n```\n\n- We have resized the image capacity, but not partition size. We can do that with:\n\n```sh\nVM$ resize2fs /dev/vda2\n```\n\n- You can check final result via\n\n```sh\nVM$ df -H\n```\n\n## No internet!\n\nNow we are going to configure our ethernet network interface.\n\n- You can check available network interfaces via:\n\n```sh\nVM$ ip addr\n```\n\n```\n1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n       valid_lft forever preferred_lft forever\n2: enp0s1: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc fq_codel state UP group default qlen 1000\n    link/ether 52:54:00:12:34:56 brd ff:ff:ff:ff:ff:ff\n```\n\n- Notice, that `enp0s1` has zero IP addresses. We gotta fix that! _By creating another file._\n\n```sh\nVM$ nano /etc/network/interfaces.d/enp0s1\n```\n\nand paste this in:\u003cbr\u003e\u003cbr\u003e\n**enp0s1**\n\n```sh\nauto enp0s1\n\niface enp0s1 inet dhcp\n```\n\n- And delete eth0 interface\n- Reboot (Although config won't actually let you reboot.)\n\n```sh\nVM$ poweroff\n```\n\n```sh\n$ ./rpistart.sh\n```\n\n- You may still receive a networking service error, but you should be able to access the internet.\n","lastmodified":"2022-09-25T20:18:24.415747785Z","tags":null},"/notes/JomOS":{"title":"JomOS","content":"\nLinks: [[notes/Linux]], [[notes/Post install optimizations]], [[notes/JomOS Optimizations]]\n\n# JomOS\n\n## About\n\nJomOS is an aggressively optimized meta Linux distribution designed for people who wants to get most out of their hardware. It allows users to mix-and-match well tested configurations and optimizations with little to no effort.\n\nJomOS integrates these configurations \u0026 optimizations into one largely cohesive system.\n\n## How does JomOS improve performance\n\nWe use tuned systctl values, udev rules and other configurations. We also provide a optimized repo with march=x86-64-v3 support (CachyOS repos) which comes with a notable performance boost. It depends on your cpu if it does support that, but you dont need to worry about it - the installer will detect the correct µarch and adjust to your system. Custom tuned kernel is also planned.\nFor more information refer to [[notes/JomOS Optimizations]].\n\n## Default browser Thorium\n\nAs far as I am aware, Thorium is the fastest browser available. It also makes use of some of the compiler optimizations we use, as well as others; for more information, see [[notes/Thorium]].\n\n## Screenshots\n\n![[notes/assets/img/distro.png]]\n\n## Credits\n\nHuge thanks to Linux community and CachyOS team for some of the optimizations and general help.\n","lastmodified":"2022-09-25T20:18:24.415747785Z","tags":null},"/notes/JomOS-Optimizations":{"title":"JomOS Optimizations","content":"\nLinks: [[notes/Linux]], [[notes/Post install optimizations]], [[notes/JomOS]]\n\n# JomOS Optimizations\n\n## Optimized repositories\n\nJomOS adds optimized repositories automatically to improve performance and system responsiveness. These repositories also include custom kernels with various CPU schedulers and other goodies.\n\nThe optimizations used in the repositories are listed below.\n\n### Compiler optimizations\n\n-march is the first and most important option. This instructs GCC(or other compilers) to generate code for a specific type of CPU. Different CPUs have different capabilities, support different instruction sets, and execute code in different ways. The -march flag instructs the compiler to generate specific code for the selected architecture, including all of its capabilities, features, instruction sets, quirks, and so on.\n\nIf the CPU type is unknown, or if the user is unsure which setting to use, the -march=native option can be used. When this flag is set, GCC will attempt to detect the processor and set appropriate flags for it automatically. This should not be used if you want to compile packages for different CPUs!\n\nWhen compiling packages on one computer to run on another (for example, when using a fast computer to build for an older, slower machine), do not use -march=native. The term \"native\" indicates that the code produced will only run on that type of CPU. Applications developed with -march=native on an Intel Core CPU will not run on an old Intel Atom CPU.\n\nThese are the four x86-64 microarchitecture levels on top of the x86-64 baseline:\n\n- x86-64: CMOV, CMPXCHG8B, FPU, FXSR, MMX, FXSR, SCE, SSE, SSE2\n- x86-64-v2: (close to Nehalem) CMPXCHG16B, LAHF-SAHF, POPCNT, SSE3, SSE4.1, SSE4.2, SSSE3\n- x86-64-v3: (close to Haswell) AVX, AVX2, BMI1, BMI2, F16C, FMA, LZCNT, MOVBE, XSAVE\n- x86-64-v4: AVX512F, AVX512BW, AVX512CD, AVX512DQ, AVX512VL\n\nMost Linux distributions use x86-64-v2 for compatibility with older hardware, but this may limit performance on newer hardware. We detect whether your CPU supports x86-64-v3 and add repositories accordingly. The performance improvement could range from 10% to 35% depending on the processor and software used.\n\n![[notes/assets/img/benchmarks.png]]\n\n![[notes/assets/img/O3_generic_O3_march_haswell_Comparison.png]]\n\nTo check if your cpu supports x86-64-v3, you can use the following command:\n`/lib/ld-linux-x86-64.so.2 --help | grep \"x86-64-v3 (supported, searched)\"`\n\nIf you get an output saying `x86-64-v3 (supported, searched)` then congratulations, your cpu supports x86-64-v3.\n\nThis repository is provided by CachyOS. As theres no reason to create our own v3 repositories. Many thanks to the CachyOS team for creating and maintaining this repository.\n\n## Tuned sysctl values and other configurations\n\n### /etc/sysctl.d/99-JomOS-settings.conf\n\nThis file contains JomOS sysctl tweaks.\n\n#### vm.swappiness\n\nThe swappiness sysctl parameter represents the kernel's preference (or avoidance) of swap space. Swappiness can have a value between 0 and 100, the default value is 60.\n\nA low value causes the kernel to avoid swapping, a higher value causes the kernel to try to use swap space. Using a low value on sufficient memory is known to improve responsiveness on many systems.\n\nThis value is automatically calculated using your ram amount\n\n#### vm.vfs_cache_pressure\n\nThe value controls the tendency of the kernel to reclaim the memory which is used for caching of directory and inode objects (VFS cache).\n\nLowering it from the default value of 100 makes the kernel less inclined to reclaim VFS cache (do not set it to 0, this may produce out-of-memory conditions)\n\nThis value is automatically calculated using your ram amount\n\n#### vm.page-cluster\n\nrefer to \u003chttps://xeome.github.io/notes/Zram#page-cluster-values-latency-difference\u003e\n\n#### vm.dirty_ratio\n\nContains, as a percentage of total available memory that contains free pages and reclaimable pages, the number of pages at which a process which is generating disk writes will itself start writing out dirty data (Default is 20).\n\n#### vm.dirty_background_ratio\n\nContains, as a percentage of total available memory that contains free pages and reclaimable pages, the number of pages at which the background kernel flusher threads will start writing out dirty data (Default is 10).\n\n#### Network tweaks (only for CachyOS kernels)\n\nThe BBR congestion control algorithm can help achieve higher bandwidths and lower latencies for internet traffic\n\nTCP Fast Open is an extension to the transmission control protocol (TCP) that helps reduce network latency by enabling data to be exchanged during the sender’s initial TCP SYN. Using the value 3 instead of the default 1 allows TCP Fast Open for both incoming and outgoing connections\n\n#### kernel.nmi_watchdog\n\nDisabling NMI watchdog will speed up your boot and shutdown, because one less module is loaded. Additionally disabling watchdog timers increases performance and lowers power consumption\n\n### /etc/udev/rules.d/ioscheduler.rules\n\nThe kernel component that determines the order in which block I/O operations are submitted to storage devices is the input/output (I/O) scheduler.The goal of the I/O scheduler is to optimize how these can deal with read requests, it is useful to review some specifications of the two main drive types:\n\n- An HDD has spinning disks and a physical head that moves to the required location. As a result, random latency is quite high, ranging between 3 and 12ms (depending on whether it is a high-end server drive or a laptop drive bypassing the disk controller write buffer), whereas sequential access provides significantly higher throughput. The average HDD throughput is approximately 200 I/O operations per second (IOPS).\n\n- An SSD does not have moving parts, random access is as fast as sequential one, typically under 0.1ms, and it can handle multiple concurrent requests. The typical SSD throughput is greater than 10,000 IOPS, which is more than needed in common workload situations.\n\nThousands of IOPS can be generated if multiple processes make I/O requests to different storage parts, whereas a typical HDD can only handle about 200 IOPS. There is a queue of requests that must wait for storage access. This is where I/O schedulers can help with optimization.\n\nThe best scheduler to use is determined by both the device and the specific nature of the workload. Furthermore, throughput in MB/s is not the only measure of performance: deadlines or fairness reduce overall throughput while improving system responsiveness.\n\n```ini\n# set scheduler for NVMe\nACTION==\"add|change\", KERNEL==\"nvme[0-9]n[0-9]\", ATTR{queue/scheduler}=\"none\"\n# set scheduler for SSD and eMMC\nACTION==\"add|change\", KERNEL==\"sd[a-z]*|mmcblk[0-9]*\", ATTR{queue/rotational}==\"0\", ATTR{queue/scheduler}=\"mq-deadline\"\n# set scheduler for rotating disks\nACTION==\"add|change\", KERNEL==\"sd[a-z]*\", ATTR{queue/rotational}==\"1\", ATTR{queue/scheduler}=\"bfq\"\n```\n\nFor example the [udev](https://wiki.archlinux.org/title/Udev \"Udev\") rule above sets the scheduler to _none_ for [NVMe](https://wiki.archlinux.org/title/NVMe \"NVMe\"), _mq-deadline_ for [SSD](https://wiki.archlinux.org/title/SSD \"SSD\")/eMMC, and _bfq_ for rotational drives:\n\n### /etc/mkinitcpio.conf\n\nBase and udev replaced with systemd for faster boots and set compression algorithm to zstd and compression level to 2 because compression ratio increase isn't worth the increased boot time.\n\n### /etc/systemd/zram-generator.conf\n\nUse zstd compression by default, for more information visit [[notes/Zram]]\n\n# Sources\n\n## Benchmarks\n\n\u003chttps://lists.archlinux.org/pipermail/arch-general/2021-March/048739.html\u003e\n\n\u003chttps://openbenchmarking.org/result/2103142-HA-UARCHLEVE55\u0026rmm=O1_generic%2CO3_march_nehalem\u003e\n","lastmodified":"2022-09-25T20:18:24.415747785Z","tags":null},"/notes/Linux":{"title":"Linux","content":"\n# Linux\n\n## WIP\n\n- [[notes/JomOS]]\n- [[notes/JomOS Optimizations]]\n- [[notes/Post install optimizations]]\n- [[notes/XDP-Tutorial]]\n- [[notes/Linux Memory Management]]\n\n## Done\n\n- [[notes/Btrfs Maintenance]]\n- [[notes/Zram]]\n- [[notes/Transparent Huge Pages]]\n","lastmodified":"2022-09-25T20:18:24.415747785Z","tags":null},"/notes/Linux-Memory-Management":{"title":"Linux Memory Management","content":"\nLinks:\n\n# Linux Memory Management\n\n## Memory Management Concepts\n\n### Virtual Memory\n\nVirtual memory abstracts the details of physical memory from the userspace software. It allows only the information needed to be held in physical memory (paging on demand) and provides a mechanism for the protected and controlled sharing of data between processes. When the CPU executes a process that reads (or writes) from (or to) system memory, it translates the virtual address encoded in that instruction into a physical address.\n\nLinux groups data in memory into pages, typically 4KiB in size. Address translation requires several memory accesses and memory accesses are slow. To avoid wasting valuable CPU cycles on address translation, CPUs keep a cache of such translations called Translation Lookaside Buffer (or TLB). Typically, the TLB is a very limited resource and applications with large memory working sets will suffer performance loss because they cannot utilize the TLB.\n\n### Transparent Huge Pages\n\nWhen the CPU assigns memory to processes that require memory, it usually does so in chunks of 4 KB pages. Since the CPU's MMU unit must actively translate virtual memory into physical memory upon incoming I/O requests, it is inherently expensive to review all 4 KB pages. Fortunately, it has its own TLB cache (translation lookaside buffer), which reduces the potential time required to access a given memory address by caching the most recently used memory. The fact that the TLB cache size is usually very limited can cause a large potential bottleneck for applications with high memory entropy.\n\nMany modern CPU architectures support direct memory page mapping via higher levels in the page table. On x86, for example, entries in the second and third level page tables can be used to map 2M and even 1G pages. In Linux, such pages are referred to as huge. The use of huge pages relieves TLB pressure, improves TLB hit-rate, and thus improves overall system performance.\n\n### LRU list\n\nA pair of least recently used (LRU) lists are used by the Linux kernel to track pages. Pages that have been recently accessed are kept in the \"active\" list, and newly accessed pages are at the top of the list. If a page has not been accessed recently, it is removed from the list's queue and moved to the top of the \"inactive\" list. When a process accesses a page in the inactive list, it is returned to the active list.\n\n### Unevictable LRU Infrastructure\n\nAn x86 64 platform with 128 GB main memory, for example, will have more than 32 million 4k pages in a single region. If the majority of these pages are unevictable, vmscan will scan the LRU lists for evictable parts, which will consume a significant amount of CPU. The system's performance will deteriorate.\n\nThe unevictable list addresses the following classes of unevictable pages:\n\n- Those owned by ramfs.\n- Those in the SHM_LOCK shared memory zones of ramfs\n- VMAs marked as VM_LOCKED (mlock()ed) (virtual memory area)\n\nFor each zone, the Unevictable LRU engine generates a separate list of LRUs. The unevictable list is referred to as such, and the PG_unevictable flag is used to indicate that pages are unevictable.\n\nThe Unevictable LRU infrastructure maintains unevictable pages on an additional LRU list for a few reasons:\n\n1. We get to “treat unevictable pages just like we treat other pages in the system - which means we get to use the same code to manipulate them, the same code to isolate them (for migrate, etc.), the same code to keep track of the statistics, etc...” - Rik van Riel\n2. We want to be able to migrate unevictable pages between nodes for memory defragmentation, workload management and memory hotplug. The linux kernel can only migrate pages that it can successfully isolate from the LRU lists. If we were to maintain pages elsewhere than on an LRU-like list, where they can be found by isolate_lru_page(), we would prevent their migration, unless we reworked migration code to find the unevictable pages itself.\n\nThe unevictable list does not differentiate between files and anonymous, swap pages. This distinction only applies when pages are evictable.\n\n# What is MGLRU\n\nThe Linux kernel has developed mechanisms designed to increase the chances of predicting which memory pages will be accessed in the near future. Yu Zhao's MGLRU (multi generational least recently used) patch set is an attempt to improve the situation. It aims to make it easier for the system to discard unused data pages in order to make room in memory for new data.\n\nA pair of least recently used (LRU) lists are used by the kernel to track pages. Pages that have been recently accessed are kept in the \"active\" list, and newly accessed pages are at the top of the list. Some pages end up in the inactive list, which means they will be reclaimed relatively quickly once they are no longer required. For various reasons, the kernel has a long history of dumping file-backed pages. This issue is especially effective on cloud systems.\n\nThe MGLRU patches attempt to address these issues with two key changes:\n\n- Add more LRU lists to cover the range of page ages between the current active and inactive lists; these lists are called \"generations\".\n- Reduce overhead by changing the way pages are scanned (the old system uses a complex reverse mapping algorithm)\n\nOnly the oldest generation should be considered when reclaiming pages. The \"oldest generation\" may differ for anonymous and file-backed pages; anonymous pages may be more difficult to reclaim in general (they must always be written to swap), and the new code retains some of the bias toward aggressively reclaiming file-backed pages. As a result, file-backed pages may not survive reclaim for as many generations as anonymous pages. However, the current patch only allows reclaiming of file-backed pages to get one generation ahead of anonymous pages.\n\n# Sources\n\n- \u003chttps://lwn.net/Articles/851184/\u003e\n- \u003chttps://www.kernel.org/doc/html/v5.0/vm/unevictable-lru.html\u003e\n- \u003chttps://docs.kernel.org/admin-guide/mm/concepts.html#mm-concepts\u003e\n","lastmodified":"2022-09-25T20:18:24.415747785Z","tags":null},"/notes/Post-install-optimizations":{"title":"Post install Optimizations","content":"\nLinks: [[notes/Linux]], [[notes/Btrfs Maintenance]], [[notes/Zram]]\n\n# Post Install Optimizations\n\n### Editing mkinitcpio.conf for faster boot times\n\nReplace udev with systemd for faster boots and set compression algorithm to zstd and compression level to 2 because compression ratio increase isn't worth the increased boot time.\n\n(bellow isnt the whole file, just the parts that needs changes)\n\n```ini\nHOOKS=\"base systemd autodetect...\n\nCOMPRESSION=\"zstd\"\nCOMPRESSION_OPTIONS=(-2)\n```\n\nNote: You can replace base AND udev with systemd but you will lose access to recovery shell.\n\n### Changing io schedulers\n\nThe process to change I/O scheduler, depending on whether the disk is rotating or not can be automated and persist across reboots. For example the udev rule below sets the scheduler to none for NVMe, mq-deadline for SSD/eMMC, and bfq for rotational drives:\n\n```ini\n# /etc/udev/rules.d/60-ioschedulers.rules\n\n# set scheduler for NVMe\nACTION==\"add|change\", KERNEL==\"nvme[0-9]n[0-9]\", ATTR{queue/scheduler}=\"none\"\n# set scheduler for SSD and eMMC\nACTION==\"add|change\", KERNEL==\"sd[a-z]*|mmcblk[0-9]*\", ATTR{queue/rotational}==\"0\", ATTR{queue/scheduler}=\"mq-deadline\"\n# set scheduler for rotating disks\nACTION==\"add|change\", KERNEL==\"sd[a-z]*\", ATTR{queue/rotational}==\"1\", ATTR{queue/scheduler}=\"bfq\"\n```\n\n### Editing /etc/makepkg.conf (in Arch linux or derivatives)\n\nEdit makepkg config file for it to utilize all threads on your cpu\nExample for 12 threads:\n\n```ini\nMAKEFLAGS=\"-j12\"\n```\n\n### Auto nice daemons and irq balance\n\nAnanicy-cpp can be installed to automatically set [nice](https://en.wikipedia.org/wiki/Nice_(Unix)) levels.\n\n[Irq balance](https://wiki.archlinux.org/title/Improving_performance#irqbalance) distributes [hardware interrupts](https://en.wikipedia.org/wiki/Interrupt_request_(PC_architecture)) across available processors to improve system performance.\n\n### Zram or Zswap\n\nZswap is a kernel feature that provides a compressed RAM cache for swap pages. Pages which would otherwise be swapped out to disk are instead compressed and stored into a memory pool in RAM. Once the pool is full or the RAM is exhausted, the least recently used (LRU) page is decompressed and written to disk, as if it had not been intercepted. After the page has been decompressed into the swap cache, the compressed version in the pool can be freed.\n\nThe difference compared to ZRAM is that zswap works in conjunction with a swap device while zram is a swap device in RAM that does not require a backing swap device.\n\nSince it is enabled by default, [disable zswap](https://wiki.archlinux.org/title/Zswap#Toggling_zswap \"Zswap\") when you use zram to avoid it acting as a swap cache in front of zram. Having both enabled also results in incorrect [zramctl](https://man.archlinux.org/man/zramctl.8) statistics as zram remains mostly unused; this is because zswap intercepts and compresses memory pages being swapped out before they can reach zram.\n\n##### Recommended configurations for zswap\n\n```C\n# echo zstd \u003e /sys/module/zswap/parameters/compressor\n\n# echo 10 \u003e /sys/module/zswap/parameters/max_pool_percent\n```\n\nAbove will change zswap settings only for current session, to make the setting changes persist add `zswap.compressor=zstd zswap.max_pool_percent=10` to your bootloader's config file for the kernel command line.\n\n`/etc/sysctl.d/99-swap-tune.conf:`\nfor ssd:\n\n```ini\nvm.page-cluster = 1 \n```\n\nfor hdd:\n\n```ini\nvm.page-cluster = 2\n```\n\n##### Recommended configurations for zram\n\n`/etc/systemd/zram-generator.conf:`\n\n```ini\n[zram0]\nzram-size = ram * 1\ncompression-algorithm = zstd\n```\n\nAbove config file is for [systemd zram generator](https://github.com/systemd/zram-generator)\n\nYou can increase `zram-size` further if you find compression ratio to be high enough.\n\n`/etc/sysctl.d/99-swap-tune.conf:`\n\n```ini\nvm.page-cluster = 0\n```\n\nA more detailed explanation can about why these values were chosen can be found in [[notes/Zram]].\n\n### Transparent Huge Pages\n\nTo summarize, transparent hugepages are a framework within the Linux kernel that allows it to automatically facilitate and allocate large memory page block sizes to processes (such as games) with sizes averaging around 2 MB per page and occasionally 1 GB (the kernel will automatically adjust the size to what the process needs).\n\n```bash\n[user@host ~]$ cat /sys/kernel/mm/transparent_hugepage/enabled\n[always] madvise never\n```\n\nThere are 3 values you can choose You should try each value yourself to see if it improves your workflow, for more information click here: [[notes/Transparent Huge Pages]].\nTo change the value for current session:\n\n```bash\necho 'always' | sudo tee /sys/kernel/mm/transparent_hugepage/enabled\n```\n\nTo make changes persist:\\\nInstall sysfsutils and then add `kernel/mm/transparent hugepage/enabled=always` to `/etc/sysfs.conf` or add `transparent_hugepage=always` to your bootloader's config file for the kernel command line.\n\n# Additional sources\n\n#### initramfs\n\n\u003chttps://wiki.archlinux.org/title/Mkinitcpio/Minimal_initramfs\u003e\n\n#### Zram\n\n\u003chttps://linuxreviews.org/Zram\u003e\n\n\u003chttps://docs.kernel.org/admin-guide/sysctl/vm.html\u003e\n\n\u003chttps://www.reddit.com/r/Fedora/comments/mzun99/new_zram_tuning_benchmarks/\u003e\n\n#### Transparent Huge Pages\n\n\u003chttps://www.kernel.org/doc/html/latest/admin-guide/mm/transhuge.html\u003e\n\n\u003chttps://access.redhat.com/solutions/46111\u003e\n\n\u003chttps://www.reddit.com/r/linux_gaming/comments/uhfjyt/underrated_advice_for_improving_gaming/\u003e\n","lastmodified":"2022-09-25T20:18:24.415747785Z","tags":null},"/notes/Recommended-Tools":{"title":"Recommended Tools","content":"\nLinks: [[notes/Linux]]\n\n# Recommended Tools\n\nThese are some of the tools and programs that I recommend and use. I'll keep adding more as I find them.\n\n## gping\n\nPing, but with graph and statistics. Works fine on tty.\n![[notes/assets/img/gping.png]]\n\n## Bat\n\nCat clone with syntax highlighting and git integration.\n![[notes/assets/img/bat.png]]\n","lastmodified":"2022-09-25T20:18:24.415747785Z","tags":null},"/notes/Thorium":{"title":"Thorium","content":"\nLinks: [[notes/VS Code Server]]\n\n# Thorium\n\nChromium fork for linux named after [radioactive element No. 90](https://en.wikipedia.org/wiki/Thorium) that takes pride in being a highly optimized web browser.\n\nThorium makes many modifications to compiler configurations which highly improve performance and responsiveness. Google tries to minimize size at any cost (including RAM usage and performance), but Thorium takes a different approach. Thorium takes up approximately ~250MB compared to ~150MB for Chrome. Me and many others appreciate speed and performance over the smallest size.\n\nYou can get Thorium from \u003chttps://github.com/Alex313031/Thorium\u003e\n\n## Compiler Optimizations\n\nThorium enables the use of numerous instruction set extensions, allowing the CPU to perform certain operations much more efficiently and quickly. For example the Chromium Project, makes extensive use of vectorizable code, so AVX (Advanced Vector Extensions) is a natural next step in performance improvement. The only reason it isn't used by default in Chromium is for compatibility: older processors (pre-2011) lack AVX capability and thus cannot run AVX-compliant programs. But don't worry if you have an old processor that lacks AVX; the creator occasionally makes SSE4/SSE3-only releases for Linux and Windows.\n\nThis represents only the tip of the iceberg. If you're interested in learning more about Thorium's performance optimizations, click [here](https://thorium.rocks/optimizations).\n\n## Some Benchmarks\n\nDepending on your operating system and hardware, performance improvements may vary. Here are some results from tests on an old CPU, an FX-8370 clocked at 4.7GHz across all cores.\n\n### Chromium\n\n![[notes/assets/img/chromium_octane.png]]\n![[notes/assets/img/chromium_speedometer.png]]\n\n### Thorium\n\n![[notes/assets/img/thorium_octane.png]]\n![[notes/assets/img/thorium_speedometer.png]]\n\nThere are significant performance improvements, but they may not be as dramatic on your device. For example, on my Ryzen 5 5500U, I get a 105 on the chromium speedometer and a 175 on the Thorium speedometer. Not a 3x score difference but still very significant.\n","lastmodified":"2022-09-25T20:18:24.415747785Z","tags":null},"/notes/Transparent-Huge-Pages":{"title":"Transparent Huge Pages","content":"\nLinks: [[notes/Linux]], [[notes/Post install optimizations]], [[notes/JomOS]]\n\n# Transparent Huge Pages\n\nWhen the CPU assigns memory to processes that require it, it typically does so in 4 KB page chunks. Because the CPU's MMU unit actively needs to translate virtual memory to physical memory upon incoming I/O requests, going through all 4 KB pages is naturally an expensive operation. Fortunately, it has its own TLB cache (translation lookaside buffer), which reduces the potential amount of time required to access a specific memory address by caching the most recently used memory. The only issue is that TLB cache size is typically very limited, and when it comes to gaming, especially playing triple AAA games, the high memory entropy nature of those applications causes a huge potential bottleneck.\n\nIn terms of the overhead that TLB lookups will incur. This is due to the technically inherent inefficiency of having a large number of entries in the page table, all with very small sizes.\n\nTo enable automatic use of transparent hugepages, first ensure that they are enabled in your kernel by running `cat /sys/kernel/mm/transparent_hugepage/enabled`. If it says error: the file or directory cannot be found, it means your kernel was built without support for it, and you must either manually build and enable the feature before compiling it or use a different kernel.\n\nThere are 3 values you can choose for transparent huge pages:\n\n### always\n\nShould be self explanatory.\n\n### madvise\n\nOnly enabled inside MADV_HUGEPAGE regions (to avoid the risk of consuming more memory resources, relevant for embedded systems).\n\n### never\n\nEntirely disabled(mostly for debugging purposes).\n\nUse `echo 'always' | sudo tee /sys/kernel/mm/transparent_hugepage/enabled` to set the value to 'always'.\n\nIt may appear that `always` is the best option, but in some cases, such as database software, it degrades performance.\nFor example mongodb docs says:\n\n\u003e Transparent Huge Pages (THP) is a Linux memory management system that reduces the overhead of Translation Lookaside Buffer (TLB) lookups on machines with large amounts of memory by using larger memory pages.\n\u003e\n\u003e However, database workloads often perform poorly with THP enabled, because they tend to have sparse rather than contiguous memory access patterns. When running MongoDB on Linux, THP should be disabled for best performance.\n\nSo you should experiment with each value to see which one works best for your workload.\n\n# Additional sources\n\n\u003chttps://www.kernel.org/doc/html/latest/admin-guide/mm/transhuge.html\u003e\n\n\u003chttps://access.redhat.com/solutions/46111\u003e\n\n\u003chttps://www.mongodb.com/docs/manual/tutorial/transparent-huge-pages/#:~:text=Transparent%20Huge%20Pages%20(THP)%20is,by%20using%20larger%20memory%20pages.\u003e\n\n\u003chttps://www.reddit.com/r/linux_gaming/comments/uhfjyt/underrated_advice_for_improving_gaming/\u003e\n","lastmodified":"2022-09-25T20:18:24.415747785Z","tags":null},"/notes/Trim":{"title":"Why do SSDs need trim","content":"\nLinks: [[notes/Btrfs Maintenance]]\n\n# Why do SSDs need trim\n\n## How does SSD store data\n\nLet's look at the structure of the SSD to understand the problems it faces and why we need TRIM operation to solve them. Data is typically stored in pages, which are groups of 4KB cells. For most SSDs, the pages are then grouped into clusters of 128 pages called Blocks, with each block containing 512KB.\n\nYou can read data from a page that contains some information or write data to clean pages (with no data from before in them). However, you cannot overwrite data on a previously written 4KB page without also overwriting the remaining 512KB.\n\nThis is because the voltages required to flip a 0 to 1 are frequently much higher than the reverse. Excess voltage has the potential to flip bits on adjacent cells and corrupt data.\n\n## File deletion\n\nWhen you delete a file the SSD simply marks all corresponding pages as invalid. Instead of being physically zeroed out, the sectors are marked as free. This significantly speeds up the deletion process.\n\nAssume you change a file, which corresponds to a single 4KB page change. When you try to modify a 4KB page in an SSD, the entire content of its block, all 512KB of it, must be read into a cache (which can be built into the SSD or use system's main memory), the block must be erased, and then you can write the new data to your target 4KB page. You will also need to restore the remaining unmodified 508KB of data from your cache.\n\n## So what does TRIM do?\n\nTRIM informs the drive that the blocks are no longer in use. That is, they can be deleted and used again for new data.\n\nAn SSD does not understand the file system that has been written to it. As a result, it has no idea how NTFS deletes files. Now TRIM comes into play. After a file is deleted, the operating system sends a TRIM command to the SSD, along with a list of sectors that should be marked free and erased.\n\nThe TRIM command reduces performance degradation by trimming invalid pages on a regular basis. Windows 10 for example TRIMs your SSD once a week. When that operation is run, the SSD controller cleans out all the data that has been marked as deleted by the OS from the memory cells. Yes, it still has to go through the read-modify-write operation, but it only happens once a week, so if all pages in a block are marked for deletion by the time trimming occurs, there will be no pages to copy to the cache therefore reducing writes and improving device lifespan.\n\nWhen you want to write to a page again, it will be empty and ready for a direct write operation!\n","lastmodified":"2022-09-25T20:18:24.415747785Z","tags":null},"/notes/VS-Code-Server":{"title":"VS Code Server","content":"\nLinks: [[notes/ZeroTier]]\n\n# VS Code Server\n\nRun [VS Code](https://github.com/Microsoft/vscode) on any machine anywhere and access it in the browser.\n\nCompilations, downloads, and other similar tasks should not be done on your primary computer. To avoid straining your current device, you can perform all of these tasks on a remote machine. Extensions can also be run remotely, consuming fewer system resources.\n\nThe default bundled electron isn't very optimized; however, with VS Code server, you can use your local web browser without having to launch another browser (electron). You can also maximize resource efficiency by using an optimized browser, such as [[notes/Thorium]].\n\nI'm running code server on my Raspberry Pi, and I can also access the pi from anywhere using [[notes/ZeroTier]].\n\nVideo Demo:\n\u003chttps://xeome.github.io/notes/assets/img/code-server.mp4\u003e\n\nFor more information visit [here](https://github.com/coder/code-server)\n","lastmodified":"2022-09-25T20:18:24.415747785Z","tags":null},"/notes/Writing-Documents":{"title":"Writing Documents","content":"\n# How i write documents on this site\nTodo","lastmodified":"2022-09-25T20:18:24.415747785Z","tags":null},"/notes/XDP-Tutorial":{"title":"XDP-Tutorial","content":"\nLinks: [[notes/Linux]]\n\n# XDP-Tutorial\n\n## Introduction\n\nXDP is an upstream Linux kernel component that allows users to install packet processing programs into the kernel. The programs are written in restricted C and compiled into eBPF byte code. Read the [the academic paper (pdf)](https://github.com/xdp-project/xdp-paper/blob/master/xdp-the-express-data-path.pdf) or the [Cilium BPF reference guide](https://cilium.readthedocs.io/en/latest/bpf/) for a general introduction to XDP.\n\nThis tutorial aims to provide a hands-on introduction to the various steps required to create useful programs with the XDP system. We assume you know the basics of Linux networking and how to configure it with the iproute2 suite of tools, but you have no prior experience with eBPF or XDP. All of the lessons are written in C, and they cover basic pointer arithmetic and aliasing. This tutorial is intended to be a hands-on introduction to the various steps required to successfully write useful programs using the XDP system.\n\nPlease keep in mind that this tutorial was written by a university first-year computer science student who has only recently begun learning XDP.\n\n## Dependencies\n\nFor basic dependencies refer to \u003chttps://github.com/xdp-project/xdp-tutorial/blob/master/setup_dependencies.org\u003e.\n\nYou will also need xdp-tools. If your distribution repositories lack xdp-tools, you can follow the build instructions from here \u003chttps://github.com/xdp-project/xdp-tools\u003e .\n\n## Examples\n\n### Example 1 - Writing a program to pass all packets\n\n```c\n#include \u003clinux/bpf.h\u003e\n#include \u003cbpf/bpf_helpers.h\u003e\n\nSEC(\"prog\")\nint xdp_prog_simple(struct xdp_md *ctx)\n{\n    return XDP_PASS;\n}\n\nchar _license[] SEC(\"license\") = \"GPL\";\n```\n\n#### Compiling and loading the example code\n\nThe LLVM+clang compiler turns this restricted-C code into BPF-byte-code and stores it in an ELF object file, named `xdp_pass.o`\n\n**Building:**\n\n`clang -O2 -g -Wall -target bpf -c xdp_pass.c -o xdp_pass.o`\n\n**Loading:**\n\n`sudo xdp-loader load -m skb -s prog interface_name xdp_pass.o`\n\nChange the interface_name to the name of your interface (for example, `eth0`, `wlan0`).\n\n**Unloading:**\n\n`sudo xdp-loader unload -a interface_name`\nAs previously described, change the interface name.\n\n### Example 2 - Blocking ICMP packets\n\n```C\n#include \u003cstdint.h\u003e\n#include \u003carpa/inet.h\u003e\n#include \u003clinux/bpf.h\u003e\n#include \u003cbpf/bpf_helpers.h\u003e\n#include \u003clinux/icmp.h\u003e\n#include \u003clinux/if_ether.h\u003e\n#include \u003clinux/ip.h\u003e\n#include \u003clinux/ipv6.h\u003e\n#include \u003clinux/tcp.h\u003e\n\n#define OVER(x, d) (x + 1 \u003e (typeof(x))d)\n\nSEC(\"prog\")\nint xdp_prog_simple(struct xdp_md *ctx)\n{\n    /* data and data_end are pointers to the beginning and end of the packet’s raw\n    memory. Note that ctx-\u003edata and ctx-\u003edata_end are of type __u32, so we have\n    to perform the casts */\n    void *data_end = (void *)(uintptr_t)ctx-\u003edata_end;\n    void *data = (void *)(uintptr_t)ctx-\u003edata;\n    \n    struct ethhdr *eth = data;\n    struct iphdr *iph = (struct iphdr *)(eth + 1);\n    struct icmphdr *icmph = (struct icmphdr *)(iph + 1);\n\n    /* sanity check needed by the eBPF verifier\n    When accessing the data in struct ethhdr, we must make sure we don't\n    access invalid areas by checking whether data + sizeof(struct ethhdr) \u003e\n    data_end, and returning without further action if it's true. This check\n    is compulsory by the BPF verifer that verifies your program at runtime. */\n    if (OVER(eth, data_end))\n        return XDP_DROP;\n\n    if (eth-\u003eh_proto != ntohs(ETH_P_IP))\n        return XDP_PASS;\n\n    /* sanity check needed by the eBPF verifier */\n    if (OVER(iph, data_end))\n        return XDP_DROP;\n\n    /* sanity check needed by the eBPF verifier */\n    if (OVER(icmph, data_end))\n        return XDP_DROP;\n\n    /* \n\tstruct iphdr {\n\t#if defined(__LITTLE_ENDIAN_BITFIELD)\n\t\t__u8\tihl:4,\n\t\t\tversion:4;\n\t#elif defined (__BIG_ENDIAN_BITFIELD)\n\t\t__u8\tversion:4,\n  \t\t\tihl:4;\n\t#else\n\t#error\t\"Please fix \u003casm/byteorder.h\u003e\"\n\t#endif\n\t\t__u8\ttos;\n\t\t__be16\ttot_len;\n\t\t__be16\tid;\n\t\t__be16\tfrag_off;\n\t\t__u8\tttl;\n\t\t__u8\tprotocol;\n\t\t__sum16\tcheck;\n\t\t__be32\tsaddr;\n\t\t__be32\tdaddr;     \n\t}; \n\tThis is the ipheader structure from ip.h; we can see the elements we can access \n    and their types. We can use iph-\u003eprotocol to determine whether an incoming \n    packet is an ICMP packet or not. */\n    if (iph-\u003eprotocol != IPPROTO_ICMP)\n        return XDP_PASS;\n\n    /* drop icmp */\n    if (iph-\u003eprotocol == IPPROTO_ICMP)\n        return XDP_DROP;\n    \n    return XDP_PASS;\n}\n\nchar _license[] SEC(\"license\") = \"GPL\";\n```\n\n### Example 3 - Recording how many ICMP packets arrived\n\nIn this example, we count the number of ICMP packets received from each individual source address and block incoming packets after the first five. So each source address can only send 5 ICMP packets.\n\n![[notes/assets/img/BPF_internals.png]]\n\nAs shown in the image we can use **eBPF maps** (Map Storage) for storing the amount of packets received. Maps are a general-purpose data structure used to store various types of data. They allow data sharing between eBPF kernel programs as well as between kernel and user-space applications.\n\nEach map type has the following attributes:\n\n```ini\n   *  type\n\n   *  maximum number of elements\n\n   *  key size in bytes\n\n   *  value size in bytes\n```\n\nExample code:\n\n```C\n#include \u003cstdint.h\u003e\n#include \u003carpa/inet.h\u003e\n#include \u003clinux/bpf.h\u003e\n#include \u003cbpf/bpf_helpers.h\u003e\n#include \u003clinux/icmp.h\u003e\n#include \u003clinux/if_ether.h\u003e\n#include \u003clinux/ip.h\u003e\n#include \u003clinux/ipv6.h\u003e\n#include \u003clinux/tcp.h\u003e\n\n#define OVER(x, d) (x + 1 \u003e (typeof(x))d)\n\n/* Creating a BPF map for counting ICMP packets as described above */\nstruct bpf_map_def SEC(\"maps\") cnt = {\n    .type = BPF_MAP_TYPE_HASH,\n    .key_size = sizeof(__be32),\n    .value_size = sizeof(long),\n    .max_entries = 65536,\n};\n\nSEC(\"prog\")\nint xdp_prog_simple(struct xdp_md *ctx)\n{\n\t/* data and data_end are pointers to the beginning and end of the packet’s raw\n    memory. Note that ctx-\u003edata and ctx-\u003edata_end are of type __u32, so we have\n    to perform the casts */\n\tvoid *data_end = (void *)(uintptr_t)ctx-\u003edata_end;\n\tvoid *data = (void *)(uintptr_t)ctx-\u003edata;\n\t\n    long *value;\n    \n    /* Define headers */\n\tstruct ethhdr *eth = data;\n\tstruct iphdr *iph = (struct iphdr *)(eth + 1);\n\tstruct icmphdr *icmph = (struct icmphdr *)(iph + 1);\n\n\t/* sanity check needed by the eBPF verifier\n    When accessing the data in struct ethhdr, we must make sure we don't\n    access invalid areas by checking whether data + sizeof(struct ethhdr) \u003e\n    data_end, and returning without further action if it's true. This check\n    is compulsory by the BPF verifer that verifies your program at runtime. */\n\n\tif (OVER(eth, data_end))\n\t\treturn XDP_DROP;\n\n\tif (eth-\u003eh_proto != ntohs(ETH_P_IP))\n\t\treturn XDP_PASS;\n\n\t/* sanity check needed by the eBPF verifier */\n\tif (OVER(iph, data_end))\n\t\treturn XDP_DROP;\n\n\t/* sanity check needed by the eBPF verifier */\n\tif (OVER(icmph, data_end))\n\t\treturn XDP_DROP;\n\n\t/* \n\tstruct iphdr {\n\t#if defined(__LITTLE_ENDIAN_BITFIELD)\n\t\t__u8\tihl:4,\n\t\t\tversion:4;\n\t#elif defined (__BIG_ENDIAN_BITFIELD)\n\t\t__u8\tversion:4,\n  \t\t\tihl:4;\n\t#else\n\t#error\t\"Please fix \u003casm/byteorder.h\u003e\"\n\t#endif\n\t\t__u8\ttos;\n\t\t__be16\ttot_len;\n\t\t__be16\tid;\n\t\t__be16\tfrag_off;\n\t\t__u8\tttl;\n\t\t__u8\tprotocol;\n\t\t__sum16\tcheck;\n\t\t__be32\tsaddr;\n\t\t__be32\tdaddr;     \n\t}; \n\tThis is the ipheader structure from ip.h; we can see the elements we can access \n    and their types. We can use iph-\u003eprotocol to determine whether an incoming \n    packet is an ICMP packet or not. */\n\n\tif (iph-\u003eprotocol != IPPROTO_ICMP)\n\t\treturn XDP_PASS;\n\n\t/* Check protocol of the packet */\n    if (iph-\u003eprotocol == IPPROTO_ICMP) {\n        /* Get source address */\n        __be32 source = iph-\u003esaddr;\n        /* Get value pointer address*/\n        value = bpf_map_lookup_elem(\u0026cnt, \u0026source);\n\n        if (value) {\n            *value += 1;\n        } else {\n            long temp = 1;\n            bpf_map_update_elem(\u0026cnt, \u0026source, \u0026temp, BPF_ANY);\n        }\n\n        if (value \u0026\u0026 *value \u003e 5)\n            return XDP_DROP;\n\n        return XDP_PASS;\n    }\n    \n\treturn XDP_PASS;\n}\n\nchar _license[] SEC(\"license\") = \"GPL\";\n```\n\n### Example 4 - Packet modification\nIn this example, we will set TTL to a pseudorandom number between 1-255.\n\n```C\n#include \u003cstdint.h\u003e\n#include \u003carpa/inet.h\u003e\n#include \u003clinux/bpf.h\u003e\n#include \u003cbpf/bpf_helpers.h\u003e\n#include \u003clinux/icmp.h\u003e\n#include \u003clinux/if_ether.h\u003e\n#include \u003clinux/ip.h\u003e\n#include \u003clinux/ipv6.h\u003e\n#include \u003clinux/tcp.h\u003e\n\n#define OVER(x, d) (x + 1 \u003e (typeof(x))d)\n\nstatic inline void csum_replace2(uint16_t *sum, uint16_t old, uint16_t new)\n{\n\tuint16_t csum = ~*sum;\n\n\tcsum += ~old;\n\tcsum += csum \u003c (uint16_t)~old;\n\n\tcsum += new;\n\tcsum += csum \u003c (uint16_t)new;\n\n\t*sum = ~csum;\n}\n\nSEC(\"prog\")\nint xdp_prog_simple(struct xdp_md *ctx)\n{\n    /* data and data_end are pointers to the beginning and end of the packet’s raw\n    memory. Note that ctx-\u003edata and ctx-\u003edata_end are of type __u32, so we have\n    to perform the casts */\n    void *data_end = (void *)(uintptr_t)ctx-\u003edata_end;\n    void *data = (void *)(uintptr_t)ctx-\u003edata;\n    uint8_t old_ttl;\n\n    struct ethhdr *eth = data;\n    struct iphdr *iph = (struct iphdr *)(eth + 1);\n    struct icmphdr *icmph = (struct icmphdr *)(iph + 1);\n\n    /* sanity check needed by the eBPF verifier\n    When accessing the data in struct ethhdr, we must make sure we don't\n    access invalid areas by checking whether data + sizeof(struct ethhdr) \u003e\n    data_end, and returning without further action if it's true. This check\n    is compulsory by the BPF verifer that verifies your program at runtime. */\n    if (OVER(eth, data_end))\n        return XDP_DROP;\n\n    if (eth-\u003eh_proto != ntohs(ETH_P_IP))\n        return XDP_PASS;\n\n    /* sanity check needed by the eBPF verifier */\n    if (OVER(iph, data_end))\n        return XDP_DROP;\n\n    /* sanity check needed by the eBPF verifier */\n    if (OVER(icmph, data_end))\n        return XDP_DROP;\n\n    /* set the TTL to a pseudorandom number 1..255 */\n    old_ttl = iph-\u003ettl;\n    iph-\u003ettl = bpf_get_prandom_u32() \u0026 0xff ?: 1;\n\n    /* recalculate the checksum, otherwise the IP stack will drop it */\n    csum_replace2(\u0026iph-\u003echeck, htons(old_ttl \u003c\u003c 8), htons(iph-\u003ettl \u003c\u003c 8));\n\n    return XDP_PASS;\n}\n\nchar _license[] SEC(\"license\") = \"GPL\";\n```\n\n## Sources\n\nMany sources have influenced this tutorial, including:\n\n- \u003chttps://github.com/xdp-project/xdp-tutorial/\u003e\n- \u003chttps://developers.redhat.com/blog/2021/04/01/get-started-with-xdp\u003e\n- \u003chttps://www.tigera.io/learn/guides/ebpf/ebpf-xdp/\u003e\n- \u003chttps://www.seekret.io/blog/a-gentle-introduction-to-xdp/\u003e\n- \u003chttps://man7.org/linux/man-pages/man2/bpf.2.html\u003e\n- \u003chttps://gist.github.com/teknoraver/b66115e3518bb1b7f3e79f52aa2c3424\u003e\n","lastmodified":"2022-09-25T20:18:24.415747785Z","tags":null},"/notes/ZeroTier":{"title":"ZeroTier","content":"\nLinks: [[notes/VS Code Server]]\n\n# ZeroTier\n\n[ZeroTier](https://www.zerotier.com/) is a secure virtual network backbone that allows multiple machines to communicate as if they were all connected to the same network. It is a peer-to-peer encrypted technology, which means that unlike traditional VPN solutions, messages are sent directly from host to host rather than via a central server or router.\n\nAll of the code is open source, and you can host the controller yourself or use the ZeroTierOne service, which has both free and paid plans. I'm currently on their free plan, which is solid, reliable, and consistent.\n","lastmodified":"2022-09-25T20:18:24.419747986Z","tags":null},"/notes/Zram":{"title":"Zram","content":"\nLinks: [[notes/Linux]], [[notes/Post install optimizations]]\n\n# Zram\n\n### Compression ratio difference\n\n| Algorithm | Cp time | Data | Compressed |  Total | Ratio |\n| :-------: | :-----: | :--: | :--------: | :----: | :---: |\n|    lzo    |  4.571s | 1.1G |   387.8M   | 409.8M | 2.689 |\n|  lzo-rle  |  4.471s | 1.1G |    388M    |  410M  | 2.682 |\n|    lz4    |  4.467s | 1.1G |   403.4M   | 426.4M | 2.582 |\n|   lz4hc   | 14.584s | 1.1G |   362.8M   | 383.2M | 2.872 |\n|    842    | 22.574s | 1.1G |   538.6M   | 570.5M | 1.929 |\n|    zstd   |  7.897s | 1.1G |   285.3M   | 298.8M | 3.961 |\n\n### Page-cluster values, latency difference\n\npage-cluster controls the number of pages up to which consecutive pages are read in from swap in a single attempt. This is the swap counterpart to page cache readahead. The mentioned consecutivity is not in terms of virtual/physical addresses, but consecutive on swap space - that means they were swapped out together.\n\nIt is a logarithmic value - setting it to zero means “1 page”, setting it to 1 means “2 pages”, setting it to 2 means “4 pages”, etc. Zero disables swap readahead completely.\n\nThe default value is three (eight pages at a time). There may be some small benefits in tuning this to a different value if your workload is swap-intensive.\n\nLower values mean lower latencies for initial faults, but at the same time extra faults and I/O delays for following faults if they would have been part of that consecutive pages readahead would have brought in.\n\n![[notes/assets/img/benchmarks_zram_throughput.png]]\n\n![[notes/assets/img/benchmarks_zram_latency.png]]\n\n## Main takeaways\n\nAs you can see zstd has highest compression ratio but is also slower (but still at acceptable speeds). However, compression ratio advantage is more important here because high compression ratio lets more of the working set fit in uncompressed memory, reducing the need for swap and improving performance.\n\n**If you're running desktop systems, I recommend running zstd with page-cluster set to 0, because the majority of the swapped data is most likely stale (old browser tabs). However, if you are running something that requires constant swapping, lz4 may be a better choice due to its higher throughput and lower latency.**\n\nWith zstd, the decompression is so slow that that there's essentially zero throughput gain from readahead. Use vm.page-cluster=0 as higher values has a huge latency cost. (This is default on [ChromeOS](https://bugs.chromium.org/p/chromium/issues/detail?id=263561#c16=) and seems to be standard practice on [Android](https://cs.android.com/search?q=page-cluster\u0026start=21).)\n\nThe default is `vm.page-cluster=3`, which is better suited for physical swap. Git blame says it was there in 2005 when the kernel switched to git, so it might even come from a time before SSDs.\n\n# Sources\n\n\u003chttps://linuxreviews.org/Zram\u003e\n\n\u003chttps://docs.kernel.org/admin-guide/sysctl/vm.html\u003e\n\n\u003chttps://www.reddit.com/r/Fedora/comments/mzun99/new_zram_tuning_benchmarks/\u003e\n","lastmodified":"2022-09-25T20:18:24.419747986Z","tags":null}}