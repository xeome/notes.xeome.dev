{"/":{"title":"ü™¥ Quartz 3.3","content":"\nMy markdown notes published using quartz.\n\nTo access all my notes, click here \u003chttps://xeome.github.io/notes/\u003e.\n","lastmodified":"2022-09-12T18:36:25.371671137Z","tags":null},"/notes/Btrfs-Maintenance":{"title":"Btrfs Maintenance","content":"\nLinks: [[notes/Linux]], [[notes/Post Install Optimizations]]\n\n# Btrfs Maintenance\n\n## Btrfs scrub\n\nScrubbing reads all data and metadata from devices and verifies checksums. It is not mandatory, but it may detect problems with faulty hardware early because it touches data that may not be in use and causes bit rot.\n\nIf there is data/metadata redundancy, such as DUP or RAID1/5/6 profiles, scrub can automatically repair the data if a good copy is available.\n\nYou can use¬†`sudo btrfs scrub start /`¬†to start the scan and¬†`sudo btrfs scrub status /`¬†to check the status of it.\n\n## Btrfs balance\n\nThe balance command can do a lot of things, but it primarily moves data in large chunks. It is used here to reclaim the space of the underutilized chunks so that it can be allocated again based on current needs.\n\nThe goal is to avoid situations in which it is impossible to allocate new metadata chunks, for example, because the entire device space is reserved for all the chunks, even though the total space occupied is smaller and the allocation should succeed.\n\nThe balance operation requires enough space to shuffle data around. By workspace, we mean device space with no filesystem chunks on it, not free space as reported by df, for example.\n\n**Expected outcome:** If all underutilized chunks are removed, the total value in the output of `btrfs fi df /path` should be lower than before. Examine the logs.\n\nThe balance command may fail due to a lack of space, but this is considered a minor error because the internal filesystem layout may prevent the command from finding enough workspace. This could be a good time to inspect the space manually.\n\n`sudo btrfs balance start --bg /path` to start the balance\n`sudo btrfs balance status /path` to check status\n\n## trimming\n\nAlthough not specifically related to btrfs, this still needs to be mentioned.\n\nThe TRIM (aka discard) operation can instruct the underlying device to optimize blocks that are not being used by the filesystem. The fstrim utility performs this task on demand.\n\nThis makes sense for SSDs or other types of storage that can translate TRIM actions into useful data (eg. thin-provisioned storage).\n\nYou can use `sudo fstrim --fstab --verbose` to run fstrim on all mounted filesystems mentioned in /etc/fstab on devices that support the discard operation.\n\n`--fstab` parameter documentation:\nOn devices that support the discard operation, trim all mounted filesystems listed in /etc/fstab. If the root filesystem is missing from the file, it is determined from the kernel command line. Other provided options, such as ---offset, --length, and --minimum, are applied to all of these devices. Errors originating from filesystems that do not support the discard operation, as well as read-only devices, autofs, and read-only filesystems, are silently ignored. Filesystems with the mount option \"X-fstrim.notrim\" are skipped.\n\n`--verbose` parameter documentation:\n\nVerbose execution. With this option fstrim will output the number of bytes passed from the filesystem down the block stack to the device for potential discard. This number is a maximum discard amount from the storage device‚Äôs perspective, because FITRIM ioctl called repeated will keep sending the same sectors for discard repeatedly.\n\n## Sources\n\n\u003chttps://github.com/kdave/btrfsmaintenance\u003e\n\n\u003chttps://man.archlinux.org/man/fstrim.8.en\u003e\n","lastmodified":"2022-09-12T18:36:25.371671137Z","tags":null},"/notes/Emulating-Cortex-A72":{"title":"Emulating Cortex A72","content":"\nLinks: [[notes/Linux]]\n\n# Starting out (Preparing for emulation)\n\n- Create a Project directory.\n\n```sh\n$ mkdir rpi_image\n$ cd rpi_image\n```\n\n- Download and decompress the Debian RasPi4 image.\n\n```sh\n$ wget https://raspi.debian.net/tested/20220808_raspi_4_bookworm.img.xz\n$ xz --decompress 20220808_raspi_4_bookworm.img.xz\n```\n\n- Using `fdisk`, determine the starting sector number.\n\n```sh\n$ fdisk -l 20220808_raspi_4_bookworm.img\n```\n\n```\nDisk 20220808_raspi_4_bookworm.img: 1,95 GiB, 2097152000 bytes, 4096000 sectors\nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisklabel type: dos\nDisk identifier: 0xf7e489e2\n\nDevice         Boot  Start     End Sectors  Size Id Type\n20220808_raspi_4_bookworm.img1        8192  819199  811008  396M  c W95 FAT32\n20220808_raspi_4_bookworm.img2      819200 4095999 3276800  1,6G 83 Linux\n```\n\n- Before we mount the image to do some stuff, we need to get an offset in order to correctly mount.\u003cbr\u003eFind the `Start` number in the second partition `$something.img2`. It's `819200` in my case. Multiply it by 512, which equals `419430400` in my case.\n\n- Create a mount directory:\n\n```sh\n$ mkdir /mnt/raspi4\n```\n\n- We can now mount image:\n\n```sh\n$ sudo mount -o offset=419430400 20220808_raspi_4_bookworm.img /mnt/raspi4\n```\n\n- Now we can extract kernel and initrd from image:\u003cbr\u003e(NOTE: We are cd'd into rpi_image directory)\n\n```sh\n$ cp /mnt/raspi4/vmlinuz .\n$ cp /mnt/raspi4/initrd.img .\n```\n\n- Finally, we need to edit `fstab` for slicker(?) mounting via QEMU\n\n```\n$ nano /mnt/raspi4/etc/fstab\n```\n\n```sh\n# The root file system has fs_passno=1 as per fstab(5) for automatic fsck.\nLABEL=RASPIROOT / ext4 rw 0 1\n# All other file systems have fs_passno=2 as per fstab(5) for automatic fsck.\nLABEL=RASPIFIRM /boot/firmware vfat rw 0 2\n```\n\n- Replace `LABEL=RASPIROOT` with `/dev/vda2`\n\n- Replace `LABEL=RASPIFIRM` with `/dev/vda1`\n\n- The file should look something like this.\n\n```sh\n# The root file system has fs_passno=1 as per fstab(5) for automatic fsck.\n/dev/vda2 / ext4 rw 0 1\n# All other file systems have fs_passno=2 as per fstab(5) for automatic fsck.\n/dev/vda1 /boot/firmware vfat rw 0 2\n```\n\n- We can now convert the image to qcow2.\n\n```sh\nqemu-img convert -f raw -O qcow2 20220808_raspi_4_bookworm.img rpi.qcow2\n```\n\n# Emulation Time!\n\n- We can finally start making our launch script.\n\n```sh\n$ nano rpistart.sh\n```\n\n**rpistart.sh**\n\n```sh\n#!/bin/bash\nscreen -mS raspberry-pi-4 \\\nsudo qemu-system-aarch64 \\\n-M virt \\\n-m 4096 -smp 4 \\\n-cpu cortex-a72 \\\n-kernel vmlinuz \\\n-initrd initrd.img \\\n-append \"root=/dev/vda2 panic=1 rootfstype=ext4 rw\" \\\n-hda rpi.qcow2 \\\n-no-reboot \\\n-nographic\n```\n\n- Paste the above into the script and save.\n\n### Some info about script\n\n- Since QEMU doesn't natively support Raspberry Pi 4(b), our only option is to virtualize Cortex A72 (Which is CPU used in Raspberry Pi 4(b)).\n- `-nographic` because _who needs graphics._\n- `screen` is used 'cuz _why not_.\n\n\u003cbr\u003e\n* Make script executable\n\n```sh\n$ chmod +x rpistart.sh\n```\n\n- __And you should_ be able to run QEMU instance._*\n- LAUNCH!\n\n```sh\n$ ./rpistart.sh\n```\n\n# Some wacky reality.\n\nYou have booted into nice Debian. Oh, btw, username is passwordless `root`.\u003cbr\u003eBut there are two problems:\n\n- Not enough space!\n- No internet!\n\n## Not enough space!\n\nVery simple. Just:\n\n- poweroff the VM.\n\n```sh\nVM$ poweroff\n```\n\n- In our `rpi_image` directory, we can resize `qcow2` image via:\n\n```sh\n$ qemu-img resize rpi.qcow2 +4G\n```\n\n```sh\n# Side note: You can also set exact size by getting rid of that + sign.\n```\n\n- Now we need to boot into VM once again:\n\n```sh\n$ ./rpistart.sh\n```\n\n- We have resized the image capacity, but not partition size. We can do that with:\n\n```sh\nVM$ resize2fs /dev/vda2\n```\n\n- You can check final result via\n\n```sh\nVM$ df -H\n```\n\n## No internet!\n\nNow we are going to configure our ethernet network interface.\n\n- You can check available network interfaces via:\n\n```sh\nVM$ ip addr\n```\n\n```\n1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n       valid_lft forever preferred_lft forever\n2: enp0s1: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc fq_codel state UP group default qlen 1000\n    link/ether 52:54:00:12:34:56 brd ff:ff:ff:ff:ff:ff\n```\n\n- Notice, that `enp0s1` has zero IP addresses. We gotta fix that! _By creating another file._\n\n```sh\nVM$ nano /etc/network/interfaces.d/enp0s1\n```\n\nand paste this in:\u003cbr\u003e\u003cbr\u003e\n**enp0s1**\n\n```sh\nauto enp0s1\n\niface enp0s1 inet dhcp\n```\n\n- And delete eth0 interface\n- Reboot (Although config won't actually let you reboot.)\n\n```sh\nVM$ poweroff\n```\n\n```sh\n$ ./rpistart.sh\n```\n\n- You may still receive a networking service error, but you should be able to access the internet.\n","lastmodified":"2022-09-12T18:36:25.371671137Z","tags":null},"/notes/JomOS":{"title":"JomOS","content":"\nLinks: [[notes/Linux]], [[notes/Post Install Optimizations]], [[notes/JomOS Settings]], [[notes/JomOS Optimizations]]\n\n# JomOS\n\n## About\n\nJomOS is an aggressively optimized meta Linux distribution designed for people who wants to get most out of their hardware. It allows users to mix-and-match well tested configurations and optimizations with little to no effort.\n\nJomOS integrates these configurations \u0026 optimizations into one largely cohesive system.\n\n## How does JomOS improve performance\n\nWe use tuned systctl values, udev rules and other configurations, refer to [[notes/JomOS Settings]].  We also provide a optimized repo with march=x86-64-v3 support (CachyOS repos) which comes with a notable performance boost. It depends on your cpu if it does support that, but you dont need to worry about it - the installer will detect the correct ¬µarch and adjust to your system. Custom tuned kernel is also planned.\nFor more information refer to [[notes/JomOS Optimizations]]\n\n## Screenshots\n\n#### XFCE\n\n![[notes/assets/img/distro.png]]\n\n#### i3-gaps\n\n![[notes/assets/img/newwp.png]]\n\n## Credits\n\nHuge thanks to Linux community and CachyOS team for some of the optimizations and general help.\n","lastmodified":"2022-09-12T18:36:25.371671137Z","tags":null},"/notes/JomOS-Optimizations":{"title":"JomOS Optimizations","content":"\nLinks: [[notes/Linux]], [[notes/Post Install Optimizations]], [[notes/JomOS Settings]], [[notes/JomOS]]\n\n# JomOS Optimizations\n\n## Huge repository with packages compiled with x86_64-v3\n\nThese are the four x86-64 microarchitecture levels on top of the x86-64 baseline:\n\n- x86-64: CMOV, CMPXCHG8B, FPU, FXSR, MMX, FXSR, SCE, SSE, SSE2\n- x86-64-v2: (close to Nehalem) CMPXCHG16B, LAHF-SAHF, POPCNT, SSE3, SSE4.1, SSE4.2, SSSE3\n- x86-64-v3: (close to Haswell) AVX, AVX2, BMI1, BMI2, F16C, FMA, LZCNT, MOVBE, XSAVE\n- x86-64-v4: AVX512F, AVX512BW, AVX512CD, AVX512DQ, AVX512VL\n\nFor compatibility with older hardware, most Linux distributions use x86-64-v2, but this may limit performance on newer hardware. You're probably running something newer than a Haswell CPU (roughly equalivent to x86-64-v3 baseline), also known as an Intel 4th generation CPU; if so, take advantage of the free performance. Depending on the type of processor and software used, the performance improvement could range from 10% to 35%.\n\nWe use CachyOS v3 repositories for this. As theres no reason to create our own v3 repositories.\n\n## Tuned sysctl and other configurations\n\nrefer to [[notes/JomOS Settings]]\n","lastmodified":"2022-09-12T18:36:25.371671137Z","tags":null},"/notes/JomOS-Settings":{"title":"JomOS Settings","content":"\nLinks: [[notes/Linux]], [[notes/JomOS]], [[notes/JomOS Optimizations]]\n\n# JomOS Settings\n\nDocumentation for configuration tweaks of JomOS\n\n## /etc/sysctl.d/99-JomOS-settings.conf\n\n### vm.swappiness\n\nThe swappiness sysctl parameter represents the kernel's preference (or avoidance) of swap space. Swappiness can have a value between 0 and 100, the default value is 60.\n\nA low value causes the kernel to avoid swapping, a higher value causes the kernel to try to use swap space. Using a low value on sufficient memory is known to improve responsiveness on many systems.\n\nThis value is automatically calculated using your ram amount\n\n### vm.vfs_cache_pressure\n\nThe value controls the tendency of the kernel to reclaim the memory which is used for caching of directory and inode objects (VFS cache).\n\nLowering it from the default value of 100 makes the kernel less inclined to reclaim VFS cache (do not set it to 0, this may produce out-of-memory conditions)\n\nThis value is automatically calculated using your ram amount\n\n### vm.page-cluster\n\nrefer to \u003chttps://xeome.github.io/notes/Zram#page-cluster-values-latency-difference\u003e\n\n### vm.dirty_ratio\n\nContains, as a percentage of total available memory that contains free pages and reclaimable pages, the number of pages at which a process which is generating disk writes will itself start writing out dirty data (Default is 20).\n\n### vm.dirty_background_ratio\n\nContains, as a percentage of total available memory that contains free pages and reclaimable pages, the number of pages at which the background kernel flusher threads will start writing out dirty data (Default is 10).\n\n### Network tweaks (only for CachyOS kernels)\n\nThe BBR congestion control algorithm can help achieve higher bandwidths and lower latencies for internet traffic\n\nTCP Fast Open is an extension to the transmission control protocol (TCP) that helps reduce network latency by enabling data to be exchanged during the sender‚Äôs initial TCP SYN. Using the value 3 instead of the default 1 allows TCP Fast Open for both incoming and outgoing connections\n\n### kernel.nmi_watchdog\n\nDisabling NMI watchdog will speed up your boot and shutdown, because one less module is loaded. Additionally disabling watchdog timers increases performance and lowers power consumption\n\n## /etc/udev/rules.d/ioscheduler.rules\n\nThe kernel component that determines the order in which block I/O operations are submitted to storage devices is the input/output (I/O) scheduler.The goal of the I/O scheduler is to optimize how these can deal with read requests, it is useful to review some specifications of the two main drive types:\n\n- An HDD has spinning disks and a physical head that moves to the required location. As a result, random latency is quite high, ranging between 3 and 12ms (depending on whether it is a high-end server drive or a laptop drive bypassing the disk controller write buffer), whereas sequential access provides significantly higher throughput. The average HDD throughput is approximately 200 I/O operations per second (IOPS).\n\n- An SSD does not have moving parts, random access is as fast as sequential one, typically under 0.1ms, and it can handle multiple concurrent requests. The typical SSD throughput is greater than 10,000 IOPS, which is more than needed in common workload situations.\n\nThousands of IOPS can be generated if multiple processes make I/O requests to different storage parts, whereas a typical HDD can only handle about 200 IOPS. There is a queue of requests that must wait for storage access. This is where I/O schedulers can help with optimization.\n\nThe best scheduler to use is determined by both the device and the specific nature of the workload. Furthermore, throughput in MB/s is not the only measure of performance: deadlines or fairness reduce overall throughput while improving system responsiveness.\n\n```ini\n# set scheduler for NVMe\nACTION==\"add|change\", KERNEL==\"nvme[0-9]n[0-9]\", ATTR{queue/scheduler}=\"none\"\n# set scheduler for SSD and eMMC\nACTION==\"add|change\", KERNEL==\"sd[a-z]*|mmcblk[0-9]*\", ATTR{queue/rotational}==\"0\", ATTR{queue/scheduler}=\"mq-deadline\"\n# set scheduler for rotating disks\nACTION==\"add|change\", KERNEL==\"sd[a-z]*\", ATTR{queue/rotational}==\"1\", ATTR{queue/scheduler}=\"bfq\"\n```\n\nFor example the¬†[udev](https://wiki.archlinux.org/title/Udev \"Udev\")¬†rule above sets the scheduler to¬†_none_¬†for¬†[NVMe](https://wiki.archlinux.org/title/NVMe \"NVMe\"),¬†_mq-deadline_¬†for¬†[SSD](https://wiki.archlinux.org/title/SSD \"SSD\")/eMMC, and¬†_bfq_¬†for rotational drives:\n\n## /etc/mkinitcpio.conf\n\nBase and udev replaced with systemd for faster boots and set compression algorithm to zstd and compression level to 2 because compression ratio increase isn't worth the increased boot time.\n\n## etc/systemd/zram-generator.conf\n\nUse zstd compression by default, for more information visit [[notes/Zram]]\n\n## Sources\n\n\u003chttps://wiki.archlinux.org/title/improving_performance#Input/output_schedulers\u003e\n\n\u003chttps://github.com/xeome/JomOS/blob/master/etc/sysctl.d/99-JomOS-settings.conf\u003e\n","lastmodified":"2022-09-12T18:36:25.371671137Z","tags":null},"/notes/Linux":{"title":"Linux","content":"\n# Linux\n\n## WIP\n\n- [[notes/JomOS]]\n- [[notes/JomOS Settings]]\n- [[notes/JomOS Optimizations]]\n- [[notes/x86_64-v3 Benchmarks]]\n- [[notes/Post Install Optimizations]]\n- [[notes/XDP-Tutorial]]\n- [[notes/Unevictable LRU Infrastructure]]\n\n## Done\n\n- [[notes/Btrfs Maintenance]]\n- [[notes/Zram]]\n- [[notes/Transparent Huge Pages]]\n","lastmodified":"2022-09-12T18:36:25.371671137Z","tags":null},"/notes/Post-Install-Optimizations":{"title":"Post install Optimizations","content":"\nLinks: [[notes/Linux]], [[notes/Btrfs Maintenance]], [[notes/JomOS Settings]], [[notes/Zram]]\n\n# Post Install Optimizations\n\n### Editing mkinitcpio.conf for faster boot times\n\nReplace udev with systemd for faster boots and set compression algorithm to zstd and compression level to 2 because compression ratio increase isn't worth the increased boot time.\n\n(bellow isnt the whole file, just the parts that needs changes)\n\n```ini\nHOOKS=\"base systemd autodetect...\n\nCOMPRESSION=\"zstd\"\nCOMPRESSION_OPTIONS=(-2)\n```\n\nNote: You can replace base AND udev with systemd but you will lose access to recovery shell.\n\n### Changing io schedulers\n\nThe process to change I/O scheduler, depending on whether the disk is rotating or not can be automated and persist across reboots. For example the udev rule below sets the scheduler to none for NVMe, mq-deadline for SSD/eMMC, and bfq for rotational drives:\n\n```ini\n# /etc/udev/rules.d/60-ioschedulers.rules\n\n# set scheduler for NVMe\nACTION==\"add|change\", KERNEL==\"nvme[0-9]n[0-9]\", ATTR{queue/scheduler}=\"none\"\n# set scheduler for SSD and eMMC\nACTION==\"add|change\", KERNEL==\"sd[a-z]*|mmcblk[0-9]*\", ATTR{queue/rotational}==\"0\", ATTR{queue/scheduler}=\"mq-deadline\"\n# set scheduler for rotating disks\nACTION==\"add|change\", KERNEL==\"sd[a-z]*\", ATTR{queue/rotational}==\"1\", ATTR{queue/scheduler}=\"bfq\"\n```\n\n### Editing /etc/makepkg.conf (in Arch linux or derivatives)\n\nEdit makepkg config file for it to utilize all threads on your cpu\nExample for 12 threads:\n\n```ini\nMAKEFLAGS=\"-j12\"\n```\n\n### Auto nice daemons and irq balance\n\nAnanicy-cpp can be installed to automatically set [nice](https://en.wikipedia.org/wiki/Nice_(Unix)) levels.\n\n[Irq balance](https://wiki.archlinux.org/title/Improving_performance#irqbalance) distributes [hardware interrupts](https://en.wikipedia.org/wiki/Interrupt_request_(PC_architecture)) across available processors to improve system performance.\n\n### Zram or Zswap\n\nZswap is a kernel feature that provides a compressed RAM cache for swap pages. Pages which would otherwise be swapped out to disk are instead compressed and stored into a memory pool in RAM. Once the pool is full or the RAM is exhausted, the least recently used (LRU) page is decompressed and written to disk, as if it had not been intercepted. After the page has been decompressed into the swap cache, the compressed version in the pool can be freed.\n\nThe difference compared to ZRAM is that zswap works in conjunction with a swap device while zram is a swap device in RAM that does not require a backing swap device.\n\nSince it is enabled by default,¬†[disable zswap](https://wiki.archlinux.org/title/Zswap#Toggling_zswap \"Zswap\")¬†when you use zram to avoid it acting as a swap cache in front of zram. Having both enabled also results in incorrect¬†[zramctl](https://man.archlinux.org/man/zramctl.8)¬†statistics as zram remains mostly unused; this is because zswap intercepts and compresses memory pages being swapped out before they can reach zram.\n\n##### Recommended configurations for zswap\n\n```C\n# echo zstd \u003e /sys/module/zswap/parameters/compressor\n\n# echo 10 \u003e /sys/module/zswap/parameters/max_pool_percent\n```\n\nAbove will change zswap settings only for current session, to make the setting changes persist add¬†`zswap.compressor=zstd zswap.max_pool_percent=10`¬†to your bootloader's config file for the kernel command line.\n\n`/etc/sysctl.d/99-swap-tune.conf:`\nfor ssd:\n\n```ini\nvm.page-cluster = 1 \n```\n\nfor hdd:\n\n```ini\nvm.page-cluster = 2\n```\n\n##### Recommended configurations for zram\n\n`/etc/systemd/zram-generator.conf:`\n\n```ini\n[zram0]\nzram-size = ram * 1\ncompression-algorithm = zstd\n```\n\nAbove config file is for [systemd zram generator](https://github.com/systemd/zram-generator)\n\nYou can increase `zram-size` further if you find compression ratio to be high enough.\n\n`/etc/sysctl.d/99-swap-tune.conf:`\n\n```ini\nvm.page-cluster = 0\n```\n\nA more detailed explanation can about why these values were chosen can be found in [[notes/Zram]].\n\n### Transparent Huge Pages\n\nTo summarize, transparent hugepages are a framework within the Linux kernel that allows it to automatically facilitate and allocate large memory page block sizes to processes (such as games) with sizes averaging around 2 MB per page and occasionally 1 GB (the kernel will automatically adjust the size to what the process needs).\n\n```bash\n[user@host ~]$ cat /sys/kernel/mm/transparent_hugepage/enabled\n[always] madvise never\n```\n\nThere are 3 values you can choose You should try each value yourself to see if it improves your workflow, for more information click here: [[notes/Transparent Huge Pages]].\nTo change the value for current session:\n\n```bash\necho 'always' | sudo tee /sys/kernel/mm/transparent_hugepage/enabled\n```\n\nTo make changes persist:\\\nInstall sysfsutils and then add¬†`kernel/mm/transparent hugepage/enabled=always`¬†to¬†`/etc/sysfs.conf`¬†or add¬†`transparent_hugepage=always`¬†to your bootloader's config file for the kernel command line.\n\n# Additional sources\n\n#### initramfs\n\n\u003chttps://wiki.archlinux.org/title/Mkinitcpio/Minimal_initramfs\u003e\n\n#### Zram\n\n\u003chttps://linuxreviews.org/Zram\u003e\n\n\u003chttps://docs.kernel.org/admin-guide/sysctl/vm.html\u003e\n\n\u003chttps://www.reddit.com/r/Fedora/comments/mzun99/new_zram_tuning_benchmarks/\u003e\n\n#### Transparent Huge Pages\n\n\u003chttps://www.kernel.org/doc/html/latest/admin-guide/mm/transhuge.html\u003e\n\n\u003chttps://access.redhat.com/solutions/46111\u003e\n\n\u003chttps://www.reddit.com/r/linux_gaming/comments/uhfjyt/underrated_advice_for_improving_gaming/\u003e\n","lastmodified":"2022-09-12T18:36:25.371671137Z","tags":null},"/notes/Recommended-Tools":{"title":"Recommended Tools","content":"\nLinks: [[notes/Linux]]\n\n# Recommended Tools\n\nThese are some of the tools and programs that I recommend and use. I'll keep adding more as I find them.\n\n## gping\n\nPing, but with graph and statistics. Works fine on tty.\n![[notes/assets/img/gping.png]]\n\n## Bat\n\nCat clone with syntax highlighting and git integration.\n![[notes/assets/img/bat.png]]\n","lastmodified":"2022-09-12T18:36:25.371671137Z","tags":null},"/notes/Transparent-Huge-Pages":{"title":"Transparent Huge Pages","content":"\nLinks: [[notes/Linux]], [[notes/Post Install Optimizations]], [[notes/JomOS]]\n\n# Transparent Huge Pages\n\nWhen the CPU assigns memory to processes that require it, it typically does so in 4 KB page chunks. Because the CPU's MMU unit actively needs to translate virtual memory to physical memory upon incoming I/O requests, going through all 4 KB pages is naturally an expensive operation. Fortunately, it has its own TLB cache (translation lookaside buffer), which reduces the potential amount of time required to access a specific memory address by caching the most recently used memory. The only issue is that TLB cache size is typically very limited, and when it comes to gaming, especially playing triple AAA games, the high memory entropy nature of those applications causes a huge potential bottleneck.\n\nIn terms of the overhead that TLB lookups will incur. This is due to the technically inherent inefficiency of having a large number of entries in the page table, all with very small sizes.\n\nTo enable automatic use of transparent hugepages, first ensure that they are enabled in your kernel by running `cat /sys/kernel/mm/transparent_hugepage/enabled`. If it says error: the file or directory cannot be found, it means your kernel was built without support for it, and you must either manually build and enable the feature before compiling it or use a different kernel.\n\nThere are 3 values you can choose for transparent huge pages:\n\n### always\n\nShould be self explanatory.\n\n### madvise\n\nOnly enabled inside MADV_HUGEPAGE regions (to avoid the risk of consuming more memory resources, relevant for embedded systems).\n\n### never\n\nEntirely disabled(mostly for debugging purposes).\n\nUse `echo 'always' | sudo tee /sys/kernel/mm/transparent_hugepage/enabled` to set the value to 'always'.\n\nIt may appear that `always` is the best option, but in some cases, such as database software, it degrades performance.\nFor example mongodb docs says:\n\n\u003e Transparent Huge Pages (THP) is a Linux memory management system that reduces the overhead of Translation Lookaside Buffer (TLB) lookups on machines with large amounts of memory by using larger memory pages.\n\u003e\n\u003e However, database workloads often perform poorly with THP enabled, because they tend to have sparse rather than contiguous memory access patterns. When running MongoDB on Linux, THP should be disabled for best performance.\n\nSo you should experiment with each value to see which one works best for your workload.\n\n# Additional sources\n\n\u003chttps://www.kernel.org/doc/html/latest/admin-guide/mm/transhuge.html\u003e\n\n\u003chttps://access.redhat.com/solutions/46111\u003e\n\n\u003chttps://www.mongodb.com/docs/manual/tutorial/transparent-huge-pages/#:~:text=Transparent%20Huge%20Pages%20(THP)%20is,by%20using%20larger%20memory%20pages.\u003e\n\n\u003chttps://www.reddit.com/r/linux_gaming/comments/uhfjyt/underrated_advice_for_improving_gaming/\u003e\n","lastmodified":"2022-09-12T18:36:25.371671137Z","tags":null},"/notes/Unevictable-LRU-Infrastructure":{"title":"Unevictable LRU Infrastructure","content":"\nLinks: [[notes/Linux]], [[notes/Zram]]\n\n# Unevictable LRU Infrastructure\n\nA non-NUMA x86 64 platform with 128GB of main memory, for example, will have over 32 million 4k pages in a single zone. When a large proportion of these pages are not evictable for any reason (see below), vmscan will spend a significant amount of time scanning the LRU lists in search of the small fraction of evictable pages. This can cause all CPUs to spend 100% of their time in vmscan for hours or days on end, rendering the system completely unresponsive.\n\nThe unevictable list addresses the following classes of unevictable pages:\n\n- Those owned by ramfs.\n- Those mapped into SHM_LOCK‚Äôd shared memory regions.\n- Those mapped into VM_LOCKED (mlock()ed) VMAs.\n\n### The Unevictable Page List\n\nThe Unevictable LRU infrastructure consists of a per-zone additional LRU list called the \"unevictable\" list and an associated page flag, PG_unevictable, to indicate that the page is managed on the unevictable list.\n\nThe PG_unevictable flag is similar to, but not the same as, the PG_active flag in that it indicates which LRU list a page is on when PG_lru is set.\n\nThe Unevictable LRU infrastructure keeps unevictable pages on an additional LRU list for a few reasons:\n\n1. We get to ‚Äútreat unevictable pages just like we treat other pages in the system - which means we get to use the same code to manipulate them, the same code to isolate them (for migrate, etc.), the same code to keep track of the statistics, etc...‚Äù -Rik van Riel\n2. We want to be able to migrate unevictable pages between nodes for memory defragmentation, workload management and memory hotplug. The linux kernel can only migrate pages that it can successfully isolate from the LRU lists. If we were to maintain pages elsewhere than on an LRU-like list, where they can be found by isolate_lru_page(), we would prevent their migration, unless we reworked migration code to find the unevictable pages itself.\n\nThe unevictable list makes no distinction between files and anonymous, swap-backed pages. This distinction is only relevant while the pages are evictable.\n\nThe unevictable list benefits from the ‚Äúarrayification‚Äù of the per-zone LRU lists and statistics originally proposed and posted by Christoph Lameter.\n\nThe LRU pagevec mechanism is not used by the unevictable list. Unevictable pages are instead added to the page's zone's unevictable list under the zone lru lock. This allows us to avoid stranding pages on the unevictable list when one task isolates the page from the LRU while other tasks change the \"evictability\" state of the page.\n\n### Memory Control Group Interaction\n\nBy extending the lru list enum, the unevictable LRU facility communicates with the memory control group (aka memory controller; see Documentation/cgroup-v1/memory.txt).\n\nAs a result of the \"arrayification\" of the per-zone LRU lists (one per lru list enum element), the memory controller data structure automatically obtains a per-zone unevictable list. The memory controller monitors page movement to and from the unevictable list.\n\nWhen memory pressure is applied to a memory control group, the controller will not attempt to reclaim pages from the unevictable list. This has the following effects:\n\n1. Because the pages on the unevictable list are \"hidden\" from reclaim, the reclaim process can be more efficient, dealing only with pages that have a chance of being reclaimed.\n\n2. However, if too many of the pages charged to the control group are unevictable, the evictable portion of the control group's working set may not fit into the available memory. The control group may thrash or OOM-kill tasks as a result of this.\n\n### Marking Address Spaces Unevictable\n\nFor facilities such as ramfs none of the pages attached to the address space may be evicted. The AS_UNEVICTABLE address space flag is provided to prevent eviction of such pages, and it can be manipulated by a filesystem using a number of wrapper functions:\n\n- `void¬†mapping_set_unevictable(struct¬†address_space¬†*mapping);`\n\n  Mark the address space as being completely unevictable.\n\n- `void¬†mapping_clear_unevictable(struct¬†address_space¬†*mapping);`\n\n  Mark the address space as being evictable.\n\n- `int¬†mapping_unevictable(struct¬†address_space¬†*mapping);`\n\n  Query the address space, and return true if it is completely unevictable.\n\n### Detecting Unevictable Pages\n\nIn vmscan.c, the function page_evictable() checks the AS_UNEVICTABLE flag to see if a page is evictable or not.\n\nFor address spaces that are so marked after being populated (as SHM regions may be), the lock action (eg: SHM_LOCK) can be lazy, and does not need to populate the page tables for the region as mlock() does, nor does it need to make any special effort to push any pages in the SHM LOCK'd area to the unevictable list. Instead, vmscan will do this if and when the pages are encountered during a reclamation scan.\n\nThe unlocker (eg: shmctl()) must scan the pages in the region and \"rescue\" them from the unevictable list if no other condition is keeping them unevictable on an unlock action (such as SHM_UNLOCK). When an unevictable region is destroyed, the pages are \"rescued\" from the unevictable list as part of the process of freeing them.\n\npage_evictable() also checks for mlocked pages by testing an additional page flag, PG_mlocked (as wrapped by PageMlocked()), which is set when a page is faulted into or found in a VM_LOCKED vma.\n\n### Vmscan's Handling of Unevictable Pages\n\nIf unevictable pages are culled in the fault path or moved to the unevictable list during mlock() or mmap(), vmscan will not encounter them until they are evictable again (via munlock(), for example) and \"rescued\" from the unevictable list. However, there may be times when we decide to leave an unevictable page on one of the regular active/inactive LRU lists for vmscan to deal with. vmscan looks for such pages in all of the shrink_{active|inactive|page}_list() functions and will \"cull\" any that it finds: that is, it diverts those pages to the zone being scanned's unevictable list.\n\nIt is possible that a page is mapped into a VM_LOCKED VMA but is not marked as PG_mlocked. Such pages will be detected when vmscan walks the reverse map in try to unmap() (). If try to unmap() returns SWAP_MLOCK, shrink_page_list() will remove the page.\n\nAfter dropping the page lock, vmscan uses putback_lru_page(), which is the opposite operation of isolate lru_page(), to put the unevictable page back on the LRU list, effectively \"culling\" it. putback_lru_page() will recheck the unevictable state of a page that it adds to the unevictable list because the condition that makes the page unevictable may change once the page is unlocked. Putback_lru_page() removes the page from the list and performs additional attempts, including the page_unevictable() test, if the page has ceased to be evictable. These extra evictability checks shouldn't happen in the majority of calls to putback_lru_page() because such a race is an uncommon occurrence and movement of pages onto the unevictable list should be uncommon.\n\n# Sources\n\n\u003chttps://www.kernel.org/doc/html/v4.19/vm/unevictable-lru.html\u003e\n\n(This page is paraphrased from kernel documentation attempting to shorten \u0026 make it easier to understand)\n","lastmodified":"2022-09-12T18:36:25.371671137Z","tags":null},"/notes/XDP-Tutorial":{"title":"XDP-Tutorial","content":"\nLinks: [[notes/Linux]]\n\n# XDP-Tutorial\n\n## Introduction\n\nXDP is an upstream Linux kernel component that allows users to install packet processing programs into the kernel. The programs are written in restricted C and compiled into eBPF byte code. Read the [the academic paper (pdf)](https://github.com/xdp-project/xdp-paper/blob/master/xdp-the-express-data-path.pdf) or the [Cilium BPF reference guide](https://cilium.readthedocs.io/en/latest/bpf/) for a general introduction to XDP.\n\nThis tutorial aims to provide a hands-on introduction to the various steps required to create useful programs with the XDP system. We assume you know the basics of Linux networking and how to configure it with the iproute2 suite of tools, but you have no prior experience with eBPF or XDP. All of the lessons are written in C, and they cover basic pointer arithmetic and aliasing. This tutorial is intended to be a hands-on introduction to the various steps required to successfully write useful programs using the XDP system.\n\nPlease keep in mind that this tutorial was written by a university first-year computer science student who has only recently begun learning XDP.\n\n## Dependencies\n\nFor basic dependencies refer to \u003chttps://github.com/xdp-project/xdp-tutorial/blob/master/setup_dependencies.org\u003e.\n\nYou will also need xdp-tools. If your distribution repositories lack xdp-tools, you can follow the build instructions from here \u003chttps://github.com/xdp-project/xdp-tools\u003e .\n\n## Examples\n\n### Example 1 - Writing a program to pass all packets\n\n```c\n#include \u003clinux/bpf.h\u003e\n#include \u003cbpf/bpf_helpers.h\u003e\n\nSEC(\"prog\")\nint xdp_prog_simple(struct xdp_md *ctx)\n{\n    return XDP_PASS;\n}\n\nchar _license[] SEC(\"license\") = \"GPL\";\n```\n\n#### Compiling and loading the example code\n\nThe LLVM+clang compiler turns this restricted-C code into BPF-byte-code and stores it in an ELF object file, named¬†`xdp_pass.o`\n\n**Building:**\n\n`clang -O2 -g -Wall -target bpf -c xdp_pass.c -o xdp_pass.o`\n\n**Loading:**\n\n`sudo xdp-loader load -m skb -s prog interface_name xdp_pass.o`\n\nChange the interface_name to the name of your interface (for example, `eth0`, `wlan0`).\n\n**Unloading:**\n\n`sudo xdp-loader unload -a interface_name`\nAs previously described, change the interface name.\n\n### Example 2 - Blocking ICMP packets\n\n```C\n#include \u003cstdint.h\u003e\n#include \u003carpa/inet.h\u003e\n#include \u003clinux/bpf.h\u003e\n#include \u003cbpf/bpf_helpers.h\u003e\n#include \u003clinux/icmp.h\u003e\n#include \u003clinux/if_ether.h\u003e\n#include \u003clinux/ip.h\u003e\n#include \u003clinux/ipv6.h\u003e\n#include \u003clinux/tcp.h\u003e\n\n#define OVER(x, d) (x + 1 \u003e (typeof(x))d)\n\nSEC(\"prog\")\nint xdp_prog_simple(struct xdp_md *ctx)\n{\n    /* data and data_end are pointers to the beginning and end of the packet‚Äôs raw\n    memory. Note that ctx-\u003edata and ctx-\u003edata_end are of type __u32, so we have\n    to perform the casts */\n    void *data_end = (void *)(uintptr_t)ctx-\u003edata_end;\n    void *data = (void *)(uintptr_t)ctx-\u003edata;\n    \n    struct ethhdr *eth = data;\n    struct iphdr *iph = (struct iphdr *)(eth + 1);\n    struct icmphdr *icmph = (struct icmphdr *)(iph + 1);\n\n    /* sanity check needed by the eBPF verifier\n    When accessing the data in struct ethhdr, we must make sure we don't\n    access invalid areas by checking whether data + sizeof(struct ethhdr) \u003e\n    data_end, and returning without further action if it's true. This check\n    is compulsory by the BPF verifer that verifies your program at runtime. */\n    if (OVER(eth, data_end))\n        return XDP_DROP;\n\n    if (eth-\u003eh_proto != ntohs(ETH_P_IP))\n        return XDP_PASS;\n\n    /* sanity check needed by the eBPF verifier */\n    if (OVER(iph, data_end))\n        return XDP_DROP;\n\n    /* sanity check needed by the eBPF verifier */\n    if (OVER(icmph, data_end))\n        return XDP_DROP;\n\n    /* \n\tstruct iphdr {\n\t#if defined(__LITTLE_ENDIAN_BITFIELD)\n\t\t__u8\tihl:4,\n\t\t\tversion:4;\n\t#elif defined (__BIG_ENDIAN_BITFIELD)\n\t\t__u8\tversion:4,\n  \t\t\tihl:4;\n\t#else\n\t#error\t\"Please fix \u003casm/byteorder.h\u003e\"\n\t#endif\n\t\t__u8\ttos;\n\t\t__be16\ttot_len;\n\t\t__be16\tid;\n\t\t__be16\tfrag_off;\n\t\t__u8\tttl;\n\t\t__u8\tprotocol;\n\t\t__sum16\tcheck;\n\t\t__be32\tsaddr;\n\t\t__be32\tdaddr;     \n\t}; \n\tThis is the ipheader structure from ip.h; we can see the elements we can access \n    and their types. We can use iph-\u003eprotocol to determine whether an incoming \n    packet is an ICMP packet or not. */\n    if (iph-\u003eprotocol != IPPROTO_ICMP)\n        return XDP_PASS;\n\n    /* drop icmp */\n    if (iph-\u003eprotocol == IPPROTO_ICMP)\n        return XDP_DROP;\n    \n    return XDP_PASS;\n}\n\nchar _license[] SEC(\"license\") = \"GPL\";\n```\n\n### Example 3 - Recording how many ICMP packets arrived\n\nIn this example, we count the number of ICMP packets received from each individual source address and block incoming packets after the first five. So each source address can only send 5 ICMP packets.\n\n![[notes/assets/img/BPF_internals.png]]\n\nAs shown in the image we can use **eBPF maps** (Map Storage) for storing the amount of packets received. Maps are a general-purpose data structure used to store various types of data. They allow data sharing between eBPF kernel programs as well as between kernel and user-space applications.\n\nEach map type has the following attributes:\n\n```ini\n   *  type\n\n   *  maximum number of elements\n\n   *  key size in bytes\n\n   *  value size in bytes\n```\n\nExample code:\n\n```C\n#include \u003cstdint.h\u003e\n#include \u003carpa/inet.h\u003e\n#include \u003clinux/bpf.h\u003e\n#include \u003cbpf/bpf_helpers.h\u003e\n#include \u003clinux/icmp.h\u003e\n#include \u003clinux/if_ether.h\u003e\n#include \u003clinux/ip.h\u003e\n#include \u003clinux/ipv6.h\u003e\n#include \u003clinux/tcp.h\u003e\n\n#define OVER(x, d) (x + 1 \u003e (typeof(x))d)\n\n/* Creating a BPF map for counting ICMP packets as described above */\nstruct bpf_map_def SEC(\"maps\") cnt = {\n    .type = BPF_MAP_TYPE_HASH,\n    .key_size = sizeof(__be32),\n    .value_size = sizeof(long),\n    .max_entries = 65536,\n};\n\nSEC(\"prog\")\nint xdp_prog_simple(struct xdp_md *ctx)\n{\n\t/* data and data_end are pointers to the beginning and end of the packet‚Äôs raw\n    memory. Note that ctx-\u003edata and ctx-\u003edata_end are of type __u32, so we have\n    to perform the casts */\n\tvoid *data_end = (void *)(uintptr_t)ctx-\u003edata_end;\n\tvoid *data = (void *)(uintptr_t)ctx-\u003edata;\n\t\n    long *value;\n    \n    /* Define headers */\n\tstruct ethhdr *eth = data;\n\tstruct iphdr *iph = (struct iphdr *)(eth + 1);\n\tstruct icmphdr *icmph = (struct icmphdr *)(iph + 1);\n\n\t/* sanity check needed by the eBPF verifier\n    When accessing the data in struct ethhdr, we must make sure we don't\n    access invalid areas by checking whether data + sizeof(struct ethhdr) \u003e\n    data_end, and returning without further action if it's true. This check\n    is compulsory by the BPF verifer that verifies your program at runtime. */\n\n\tif (OVER(eth, data_end))\n\t\treturn XDP_DROP;\n\n\tif (eth-\u003eh_proto != ntohs(ETH_P_IP))\n\t\treturn XDP_PASS;\n\n\t/* sanity check needed by the eBPF verifier */\n\tif (OVER(iph, data_end))\n\t\treturn XDP_DROP;\n\n\t/* sanity check needed by the eBPF verifier */\n\tif (OVER(icmph, data_end))\n\t\treturn XDP_DROP;\n\n\t/* \n\tstruct iphdr {\n\t#if defined(__LITTLE_ENDIAN_BITFIELD)\n\t\t__u8\tihl:4,\n\t\t\tversion:4;\n\t#elif defined (__BIG_ENDIAN_BITFIELD)\n\t\t__u8\tversion:4,\n  \t\t\tihl:4;\n\t#else\n\t#error\t\"Please fix \u003casm/byteorder.h\u003e\"\n\t#endif\n\t\t__u8\ttos;\n\t\t__be16\ttot_len;\n\t\t__be16\tid;\n\t\t__be16\tfrag_off;\n\t\t__u8\tttl;\n\t\t__u8\tprotocol;\n\t\t__sum16\tcheck;\n\t\t__be32\tsaddr;\n\t\t__be32\tdaddr;     \n\t}; \n\tThis is the ipheader structure from ip.h; we can see the elements we can access \n    and their types. We can use iph-\u003eprotocol to determine whether an incoming \n    packet is an ICMP packet or not. */\n\n\tif (iph-\u003eprotocol != IPPROTO_ICMP)\n\t\treturn XDP_PASS;\n\n\t/* Check protocol of the packet */\n    if (iph-\u003eprotocol == IPPROTO_ICMP) {\n        /* Get source address */\n        __be32 source = iph-\u003esaddr;\n        /* Get value pointer address*/\n        value = bpf_map_lookup_elem(\u0026cnt, \u0026source);\n\n        if (value) {\n            *value += 1;\n        } else {\n            long temp = 1;\n            bpf_map_update_elem(\u0026cnt, \u0026source, \u0026temp, BPF_ANY);\n        }\n\n        if (value \u0026\u0026 *value \u003e 5)\n            return XDP_DROP;\n\n        return XDP_PASS;\n    }\n    \n\treturn XDP_PASS;\n}\n\nchar _license[] SEC(\"license\") = \"GPL\";\n```\n\n### Example 4 - Packet modification\nIn this example, we will set TTL to a pseudorandom number between 1-255.\n\n```C\n#include \u003cstdint.h\u003e\n#include \u003carpa/inet.h\u003e\n#include \u003clinux/bpf.h\u003e\n#include \u003cbpf/bpf_helpers.h\u003e\n#include \u003clinux/icmp.h\u003e\n#include \u003clinux/if_ether.h\u003e\n#include \u003clinux/ip.h\u003e\n#include \u003clinux/ipv6.h\u003e\n#include \u003clinux/tcp.h\u003e\n\n#define OVER(x, d) (x + 1 \u003e (typeof(x))d)\n\nstatic inline void csum_replace2(uint16_t *sum, uint16_t old, uint16_t new)\n{\n\tuint16_t csum = ~*sum;\n\n\tcsum += ~old;\n\tcsum += csum \u003c (uint16_t)~old;\n\n\tcsum += new;\n\tcsum += csum \u003c (uint16_t)new;\n\n\t*sum = ~csum;\n}\n\nSEC(\"prog\")\nint xdp_prog_simple(struct xdp_md *ctx)\n{\n    /* data and data_end are pointers to the beginning and end of the packet‚Äôs raw\n    memory. Note that ctx-\u003edata and ctx-\u003edata_end are of type __u32, so we have\n    to perform the casts */\n    void *data_end = (void *)(uintptr_t)ctx-\u003edata_end;\n    void *data = (void *)(uintptr_t)ctx-\u003edata;\n    uint8_t old_ttl;\n\n    struct ethhdr *eth = data;\n    struct iphdr *iph = (struct iphdr *)(eth + 1);\n    struct icmphdr *icmph = (struct icmphdr *)(iph + 1);\n\n    /* sanity check needed by the eBPF verifier\n    When accessing the data in struct ethhdr, we must make sure we don't\n    access invalid areas by checking whether data + sizeof(struct ethhdr) \u003e\n    data_end, and returning without further action if it's true. This check\n    is compulsory by the BPF verifer that verifies your program at runtime. */\n    if (OVER(eth, data_end))\n        return XDP_DROP;\n\n    if (eth-\u003eh_proto != ntohs(ETH_P_IP))\n        return XDP_PASS;\n\n    /* sanity check needed by the eBPF verifier */\n    if (OVER(iph, data_end))\n        return XDP_DROP;\n\n    /* sanity check needed by the eBPF verifier */\n    if (OVER(icmph, data_end))\n        return XDP_DROP;\n\n    /* set the TTL to a pseudorandom number 1..255 */\n    old_ttl = iph-\u003ettl;\n    iph-\u003ettl = bpf_get_prandom_u32() \u0026 0xff ?: 1;\n\n    /* recalculate the checksum, otherwise the IP stack will drop it */\n    csum_replace2(\u0026iph-\u003echeck, htons(old_ttl \u003c\u003c 8), htons(iph-\u003ettl \u003c\u003c 8));\n\n    return XDP_PASS;\n}\n\nchar _license[] SEC(\"license\") = \"GPL\";\n```\n\n## Sources\n\nMany sources have influenced this tutorial, including:\n\n- \u003chttps://github.com/xdp-project/xdp-tutorial/\u003e\n- \u003chttps://developers.redhat.com/blog/2021/04/01/get-started-with-xdp\u003e\n- \u003chttps://www.tigera.io/learn/guides/ebpf/ebpf-xdp/\u003e\n- \u003chttps://www.seekret.io/blog/a-gentle-introduction-to-xdp/\u003e\n- \u003chttps://man7.org/linux/man-pages/man2/bpf.2.html\u003e\n- \u003chttps://gist.github.com/teknoraver/b66115e3518bb1b7f3e79f52aa2c3424\u003e\n","lastmodified":"2022-09-12T18:36:25.375671164Z","tags":null},"/notes/Zram":{"title":"Zram","content":"\nLinks: [[notes/Linux]], [[notes/Post Install Optimizations]], [[notes/JomOS Settings]]\n\n# Zram\n\n### Compression ratio difference\n\n| Algorithm | Cp time | Data | Compressed |  Total | Ratio |\n| :-------: | :-----: | :--: | :--------: | :----: | :---: |\n|    lzo    |  4.571s | 1.1G |   387.8M   | 409.8M | 2.689 |\n|  lzo-rle  |  4.471s | 1.1G |    388M    |  410M  | 2.682 |\n|    lz4    |  4.467s | 1.1G |   403.4M   | 426.4M | 2.582 |\n|   lz4hc   | 14.584s | 1.1G |   362.8M   | 383.2M | 2.872 |\n|    842    | 22.574s | 1.1G |   538.6M   | 570.5M | 1.929 |\n|    zstd   |  7.897s | 1.1G |   285.3M   | 298.8M | 3.961 |\n\n### Page-cluster values, latency difference\n\npage-cluster controls the number of pages up to which consecutive pages are read in from swap in a single attempt. This is the swap counterpart to page cache readahead. The mentioned consecutivity is not in terms of virtual/physical addresses, but consecutive on swap space - that means they were swapped out together.\n\nIt is a logarithmic value - setting it to zero means ‚Äú1 page‚Äù, setting it to 1 means ‚Äú2 pages‚Äù, setting it to 2 means ‚Äú4 pages‚Äù, etc. Zero disables swap readahead completely.\n\nThe default value is three (eight pages at a time). There may be some small benefits in tuning this to a different value if your workload is swap-intensive.\n\nLower values mean lower latencies for initial faults, but at the same time extra faults and I/O delays for following faults if they would have been part of that consecutive pages readahead would have brought in.\n\n![[notes/assets/img/benchmarks_zram_throughput.png]]\n\n![[notes/assets/img/benchmarks_zram_latency.png]]\n\n## Main takeaways\n\nAs you can see zstd has highest compression ratio but is also slower (but still at acceptable speeds). However, compression ratio advantage is more important here because high compression ratio lets more of the working set fit in uncompressed memory, reducing the need for swap and improving performance.\n\nWith zstd, the decompression is so slow that that there's essentially zero throughput gain from readahead. Use vm.page-cluster=0 as higher values has a huge latency cost. (This is default on [ChromeOS](https://bugs.chromium.org/p/chromium/issues/detail?id=263561#c16=) and seems to be standard practice on [Android](https://cs.android.com/search?q=page-cluster\u0026start=21).)\n\nThe default is¬†`vm.page-cluster=3`, which is better suited for physical swap. Git blame says it was there in 2005 when the kernel switched to git, so it might even come from a time before SSDs.\n\n# Sources\n\n\u003chttps://linuxreviews.org/Zram\u003e\n\n\u003chttps://docs.kernel.org/admin-guide/sysctl/vm.html\u003e\n\n\u003chttps://www.reddit.com/r/Fedora/comments/mzun99/new_zram_tuning_benchmarks/\u003e\n","lastmodified":"2022-09-12T18:36:25.375671164Z","tags":null},"/notes/x86_64-v3-Benchmarks":{"title":"x86_64-v3 Benchmarks","content":"\nLinks: [[notes/Linux]], [[notes/Post Install Optimizations]], [[notes/JomOS]]\n\n# x86_64-v3 Benchmarks\n\n![[notes/assets/img/benchmarks.png]]\n\n![[notes/assets/img/O3_generic_O3_march_haswell_Comparison.png]]\n\n![[notes/assets/img/O3_generic_O3_march_haswell_Comparison_scimark.png]]\n\n# Sources\n\n\u003chttps://lists.archlinux.org/pipermail/arch-general/2021-March/048739.html\u003e\n\n\u003chttps://openbenchmarking.org/result/2103142-HA-UARCHLEVE55\u0026rmm=O1_generic%2CO3_march_nehalem\u003e\n","lastmodified":"2022-09-12T18:36:25.383671219Z","tags":null}}