<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Modern CPUs increasingly rely on parallelism to achieve peak performance. The most well-known form is task parallelism, which is supported at the hardware level by multiple cores, hyperthreading and dedicated instructions supporting multitasking operating systems."><meta property="og:title" content="Improving Performance With SIMD intrinsics"><meta property="og:description" content="Modern CPUs increasingly rely on parallelism to achieve peak performance. The most well-known form is task parallelism, which is supported at the hardware level by multiple cores, hyperthreading and dedicated instructions supporting multitasking operating systems."><meta property="og:type" content="website"><meta property="og:image" content="https://xeome.github.io/icon.png"><meta property="og:url" content="https://xeome.github.io/notes/SIMD-Intrinsics/"><meta property="og:width" content="200"><meta property="og:height" content="200"><meta name=twitter:card content="summary"><meta name=twitter:title content="Improving Performance With SIMD intrinsics"><meta name=twitter:description content="Modern CPUs increasingly rely on parallelism to achieve peak performance. The most well-known form is task parallelism, which is supported at the hardware level by multiple cores, hyperthreading and dedicated instructions supporting multitasking operating systems."><meta name=twitter:image content="https://xeome.github.io/icon.png"><title>Improving Performance With SIMD intrinsics</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://xeome.github.io//icon.png><link href=https://xeome.github.io/styles.ec4dbc0520be7bdf4aca4fa74d672b05.min.css rel=stylesheet><link href=https://xeome.github.io/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://xeome.github.io/js/darkmode.7f967d9bd2d1cac02e762b5c57efc6ae.min.js></script>
<script src=https://xeome.github.io/js/util.a0ccf91e1937fe761a74da4946452710.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/core@1.2.1></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/dom@1.2.1></script>
<script async src=https://xeome.github.io/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://xeome.github.io/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://xeome.github.io/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://xeome.github.io/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://xeome.github.io/",fetchData=Promise.all([fetch("https://xeome.github.io/indices/linkIndex.e32816f04485598f12d09a83b884f068.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://xeome.github.io/indices/contentIndex.6053416280075e2beb2d8edf07f3b100.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://xeome.github.io",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!0;drawGraph("https://xeome.github.io",t,[{"/moc":"#4388cc"}],t?{centerForce:.5,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:4,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'â€™':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/xeome.github.io\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=xeome.github.io src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://xeome.github.io/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a class=root-title href=https://xeome.github.io/>ðŸª´ Quartz 3.3</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Improving Performance With SIMD intrinsics</h1><p class=meta>Last updated
Mar 1, 2023
<a href=https://github.com/xeome/xeome.github.io/tree/hugo/content/notes/SIMD%20Intrinsics.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#concepts>Concepts</a></li></ol><ol><li><a href=#basic-addition>Basic Addition</a></li><li><a href=#dot-product>Dot Product</a><ol><li><a href=#one-accumulator-no-fma>One accumulator, no FMA</a></li><li><a href=#why-multiple-accumulators>Why multiple accumulators?</a></li></ol></li></ol></nav></details></aside><blockquote><p>Modern CPUs increasingly rely on parallelism to achieve peak performance. The most well-known form is task parallelism, which is supported at the hardware level by multiple cores, hyperthreading and dedicated instructions supporting multitasking operating systems. Less known is the parallelism known as instruction level parallelism: the capability of a CPU to execute multiple instructions simultaneously, i.e., in the same cycle(s), in a single thread. Older CPUs such as the original Pentium used this to execute instructions utilizing two pipelines, concurrently with high-latency floating point operations. Typically, this happens transparent to the programmer. Recent CPUs use a radically different form of instruction level parallelism. These CPUs deploy a versatile set of vector operations: instructions that operate on 4 or 8 inputs1 , yielding 4 or 8 results, often in a single cycle. This is known as SIMD: Single Instruction, Multiple Data. To leverage this compute potential, we can no longer rely on the compiler. Algorithms that exhibit extensive data parallelism benefit most from explicit SIMD programming, with potential performance gains of 4x - 8x and more.</p></blockquote><p>Outside of niche areas like high-performance computing, game development, or compiler development, even very experienced C and C++ programmers are largely unfamiliar with SIMD intrinsics.</p><a href=#concepts><h2 id=concepts><span class=hanchor arialabel=Anchor># </span>Concepts</h2></a><p><img src="/notes/assets/img/O_Pasted image 20230216230406.png" width=auto></p><p>Registers are used by a CPU to store data for operations. A typical register holds 32 or 64 bits. Some of these registers can be split in to 16bit parts or even single bytes.</p><p>Vector registers store 4 (SSE) or 8 (AVX) scalars. This means that the C# or C++ vector remains a vector at the assembler level: rather than storing 4 separate values in 4 registers, we store 4 values in a single vector register. And rather than operating on a, b, c and d separately, we use a single SIMD instruction to perform addition (for example) to all 4 values.</p><p>If you&rsquo;re a C++ programmer, you&rsquo;re probably familiar with the basic types like char, short, int, and float. Each of these has a different size: A char has 8 bits, a short has 16, an int has 32, and a float has 32. Because bits are just bits, the only difference between a float and an int is in the interpretation. This enables us to do some nasty things:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Cpp data-lang=Cpp><span class=line><span class=cl><span class=kt>int</span> <span class=n>a</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=kt>float</span><span class=o>&amp;</span> <span class=n>b</span> <span class=o>=</span> <span class=p>(</span><span class=kt>float</span><span class=o>&amp;</span><span class=p>)</span><span class=n>a</span><span class=p>;</span>
</span></span></code></pre></td></tr></table></div></div><p>This creates one integer and a float reference to a. Because variables a and b now share the same memory location, changing one changes the other. An alternative way to achieve this is using a union:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-C data-lang=C><span class=line><span class=cl><span class=k>union</span> <span class=p>{</span> <span class=kt>int</span> <span class=n>a</span><span class=p>;</span> <span class=kt>float</span> <span class=n>b</span><span class=p>;</span> <span class=p>};</span>
</span></span></code></pre></td></tr></table></div></div><p>Again, a and b reside in the same memory location. Hereâ€™s another example:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-C data-lang=C><span class=line><span class=cl><span class=k>union</span> <span class=p>{</span> <span class=kt>unsigned</span> <span class=kt>int</span> <span class=n>a4</span><span class=p>;</span> <span class=kt>unsigned</span> <span class=kt>char</span> <span class=n>a</span><span class=p>[</span><span class=mi>4</span><span class=p>];</span> <span class=p>};</span>
</span></span></code></pre></td></tr></table></div></div><p>This time, a small array of four chars overlaps the 32-bit integer value a4. We can now access the individual bytes in a4 via array a[4]. Note that a4 now basically has four 1-byte â€˜lanesâ€™, which is somewhat similar to what we get with SIMD. We could even use a4 as 32 1-bit values, which is an efficient way to store 32 boolean values.</p><p>An SSE register is 128 bits in size and is labeled <code>__m128</code> if it stores four floats or <code>__m128i</code> if it stores ints. For simplicity, we will refer to <code>__m128</code> as &lsquo;quadfloat&rsquo; and <code>__m128i</code> as &lsquo;quadint&rsquo;. <code>__m256</code> (&lsquo;octfloat&rsquo;) and <code>__m256i</code> (&lsquo;octint&rsquo;) are the AVX versions. To use the SIMD types, we need to include the following headers:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-C data-lang=C><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&#34;nmmintrin.h&#34; // for SSE4.2</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&#34;immintrin.h&#34; // for AVX</span><span class=cp>
</span></span></span></code></pre></td></tr></table></div></div><p>A <code>__m128</code> variable contains four floats, so we can use the union trick again:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-C data-lang=C><span class=line><span class=cl><span class=k>union</span> <span class=p>{</span> <span class=kr>__m128</span> <span class=n>a4</span><span class=p>;</span> <span class=kt>float</span> <span class=n>a</span><span class=p>[</span><span class=mi>4</span><span class=p>];</span> <span class=p>};</span>
</span></span></code></pre></td></tr></table></div></div><p>Now we can conveniently access the individual floats in the <code>__m128</code> vector. We can also create the quadfloat directly:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-C data-lang=C><span class=line><span class=cl><span class=kr>__m128</span> <span class=n>a4</span> <span class=o>=</span> <span class=n>_mm_set_ps</span><span class=p>(</span> <span class=mf>4.0f</span><span class=p>,</span> <span class=mf>4.1f</span><span class=p>,</span> <span class=mf>4.2f</span><span class=p>,</span> <span class=mf>4.3f</span> <span class=p>);</span>
</span></span><span class=line><span class=cl><span class=kr>__m128</span> <span class=n>b4</span> <span class=o>=</span> <span class=n>_mm_set_ps</span><span class=p>(</span> <span class=mf>1.0f</span><span class=p>,</span> <span class=mf>1.0f</span><span class=p>,</span> <span class=mf>1.0f</span><span class=p>,</span> <span class=mf>1.0f</span> <span class=p>);</span>
</span></span></code></pre></td></tr></table></div></div><p>To add them together, we use <code>__mm_add_ps</code>:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-C data-lang=C><span class=line><span class=cl><span class=kr>__m128</span> <span class=n>sum4</span> <span class=o>=</span> <span class=n>_mm_add_ps</span><span class=p>(</span> <span class=n>a4</span><span class=p>,</span> <span class=n>b4</span> <span class=p>);</span>
</span></span></code></pre></td></tr></table></div></div><p>The <code>__mm_set_ps</code> and <code>_mm_add_ps</code> keywords are called intrinsics. SSE and AVX intrinsics all compile to a single assembler instruction; using these means that we are essentially writing assembler code directly in our program.</p><a href=#examples><h1 id=examples><span class=hanchor arialabel=Anchor># </span>Examples</h1></a><a href=#basic-addition><h2 id=basic-addition><span class=hanchor arialabel=Anchor># </span>Basic Addition</h2></a><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-C data-lang=C><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;immintrin.h&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;stdio.h&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp></span>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=nf>main</span><span class=p>(</span><span class=kt>int</span> <span class=n>argc</span><span class=p>,</span> <span class=kt>char</span><span class=o>**</span> <span class=n>argv</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=kt>int</span> <span class=n>N</span> <span class=o>=</span> <span class=mi>16</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kt>float</span> <span class=n>a</span><span class=p>[</span><span class=n>N</span><span class=p>],</span> <span class=n>b</span><span class=p>[</span><span class=n>N</span><span class=p>],</span> <span class=n>c</span><span class=p>[</span><span class=n>N</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Initialize arrays
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>a</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=p>(</span><span class=kt>float</span><span class=p>)</span><span class=n>i</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>b</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=mf>2.0f</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Perform element-wise addition using vector instructions
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>__m256</span> <span class=n>a_vec</span><span class=p>,</span> <span class=n>b_vec</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>;</span> <span class=n>i</span> <span class=o>+=</span> <span class=mi>8</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>a_vec</span> <span class=o>=</span> <span class=n>_mm256_load_ps</span><span class=p>(</span><span class=o>&amp;</span><span class=n>a</span><span class=p>[</span><span class=n>i</span><span class=p>]);</span>
</span></span><span class=line><span class=cl>        <span class=n>b_vec</span> <span class=o>=</span> <span class=n>_mm256_load_ps</span><span class=p>(</span><span class=o>&amp;</span><span class=n>b</span><span class=p>[</span><span class=n>i</span><span class=p>]);</span>
</span></span><span class=line><span class=cl>        <span class=n>_mm256_store_ps</span><span class=p>(</span><span class=o>&amp;</span><span class=n>c</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=n>_mm256_add_ps</span><span class=p>(</span><span class=n>a_vec</span><span class=p>,</span> <span class=n>b_vec</span><span class=p>));</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Print the result
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>printf</span><span class=p>(</span><span class=s>&#34;%f + %f = %f</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>a</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=n>b</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=n>c</span><span class=p>[</span><span class=n>i</span><span class=p>]);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>This code performs element-wise addition of two arrays a and b, and stores the result in c. The addition is performed using vector instructions from the AVX instruction set, which allows us to operate on 8 elements at a time. This can result in significant performance improvements compared to a scalar implementation, especially on systems with support for hardware acceleration of vector instructions.</p><a href=#dot-product><h2 id=dot-product><span class=hanchor arialabel=Anchor># </span>Dot Product</h2></a><a href=#one-accumulator-no-fma><h3 id=one-accumulator-no-fma><span class=hanchor arialabel=Anchor># </span>One accumulator, no FMA</h3></a><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-C data-lang=C><span class=line><span class=cl><span class=kt>float</span> <span class=nf>dotProduct</span><span class=p>(</span><span class=k>const</span> <span class=kt>float</span><span class=o>*</span> <span class=n>p1</span><span class=p>,</span> <span class=k>const</span> <span class=kt>float</span><span class=o>*</span> <span class=n>p2</span><span class=p>,</span> <span class=n>size_t</span> <span class=n>count</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>count</span> <span class=o>%</span> <span class=mi>8</span> <span class=o>!=</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=mf>0.0f</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>__m256</span> <span class=n>acc</span> <span class=o>=</span> <span class=n>_mm256_setzero_ps</span><span class=p>();</span>
</span></span><span class=line><span class=cl>    <span class=c1>// p1End points to the end of the array, so that we don&#39;t process past the end.
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>const</span> <span class=kt>float</span><span class=o>*</span> <span class=k>const</span> <span class=n>p1End</span> <span class=o>=</span> <span class=n>p1</span> <span class=o>+</span> <span class=n>count</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(;</span> <span class=n>p1</span> <span class=o>&lt;</span> <span class=n>p1End</span><span class=p>;</span> <span class=n>p1</span> <span class=o>+=</span> <span class=mi>8</span><span class=p>,</span> <span class=n>p2</span> <span class=o>+=</span> <span class=mi>8</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=c1>// Load 2 vectors, 8 floats / each
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=k>const</span> <span class=n>__m256</span> <span class=n>a</span> <span class=o>=</span> <span class=n>_mm256_loadu_ps</span><span class=p>(</span><span class=n>p1</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=k>const</span> <span class=n>__m256</span> <span class=n>b</span> <span class=o>=</span> <span class=n>_mm256_loadu_ps</span><span class=p>(</span><span class=n>p2</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=c1>// vdpps AVX instruction does not compute dot product of 8-wide vectors.
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=c1>// Instead, that instruction computes 2 independent dot products of
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=c1>// 4-wide vectors.
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=k>const</span> <span class=n>__m256</span> <span class=n>dp</span> <span class=o>=</span> <span class=n>_mm256_dp_ps</span><span class=p>(</span><span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>,</span> <span class=mh>0xFF</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=n>acc</span> <span class=o>=</span> <span class=n>_mm256_add_ps</span><span class=p>(</span><span class=n>acc</span><span class=p>,</span> <span class=n>dp</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Add the 2 results into a single float.
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>const</span> <span class=kr>__m128</span> <span class=n>low</span> <span class=o>=</span> <span class=n>_mm256_castps256_ps128</span><span class=p>(</span><span class=n>acc</span><span class=p>);</span>     <span class=c1>//&lt; Compiles into no instructions. The low half of a YMM register
</span></span></span><span class=line><span class=cl><span class=c1></span>                                                        <span class=c1>// is directly accessible as an XMM register with the same
</span></span></span><span class=line><span class=cl><span class=c1></span>                                                        <span class=c1>// number.
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>const</span> <span class=kr>__m128</span> <span class=n>high</span> <span class=o>=</span> <span class=n>_mm256_extractf128_ps</span><span class=p>(</span><span class=n>acc</span><span class=p>,</span> <span class=mi>1</span><span class=p>);</span>  <span class=c1>//&lt; This one however does need to move data, from high half of a
</span></span></span><span class=line><span class=cl><span class=c1></span>                                                        <span class=c1>// register into low half. vextractf128 instruction does that.
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>// Add the first element of two __m128 vectors (low and high)
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>const</span> <span class=kr>__m128</span> <span class=n>result</span> <span class=o>=</span> <span class=n>_mm_add_ss</span><span class=p>(</span><span class=n>low</span><span class=p>,</span> <span class=n>high</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=c1>// By the way, the intrinsic below compiles into no instructions.  
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=c1>// When a function is returning a float, modern compilers pass the return value in the lowest lane of xmm0 vector register.
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>return</span> <span class=n>_mm_cvtss_f32</span><span class=p>(</span><span class=n>result</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>This function calculates dot product without using FMA(Fused Multipy and Add) instructions and using a single accumulator.</p><a href=#why-multiple-accumulators><h3 id=why-multiple-accumulators><span class=hanchor arialabel=Anchor># </span>Why multiple accumulators?</h3></a><blockquote><p>Data dependencies is the main thing Iâ€™d like to illustrate with this example.</p></blockquote><blockquote><p>From a computer scientist point of view, dot product is a form ofÂ 
<a href=https://en.wikipedia.org/wiki/Reduce_%28parallel_pattern%29 rel=noopener>reduction</a>.Â 
<a href=https://en.wikipedia.org/wiki/Reduce_%28parallel_pattern%29 rel=noopener></a>The algorithm needs to process large input vectors, and compute just a single value. When the computations are fast (like in this case, multiplying floats from sequential blocks of memory is very fast), the throughput is often limited by latency of the reduce operation.</p></blockquote><p>Letâ€™s compare code of two specific versions,Â <code>AvxVerticalFma</code>Â andÂ <code>AvxVerticalFma2</code>. The former has the following main loop:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-C data-lang=C><span class=line><span class=cl><span class=k>for</span> <span class=p>(;</span> <span class=n>p1</span> <span class=o>&lt;</span> <span class=n>p1End</span><span class=p>;</span> <span class=n>p1</span> <span class=o>+=</span> <span class=mi>8</span><span class=p>,</span> <span class=n>p2</span> <span class=o>+=</span> <span class=mi>8</span><span class=p>)</span> <span class=p>{</span>  
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=n>__m256</span> <span class=n>a</span> <span class=o>=</span> <span class=n>__mm256_loadu_ps_</span><span class=p>(</span><span class=n>p1</span><span class=p>);</span>  
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=n>__m256</span> <span class=n>b</span> <span class=o>=</span> <span class=n>__mm256_loadu_ps_</span><span class=p>(</span><span class=n>p2</span><span class=p>);</span>  
</span></span><span class=line><span class=cl>    <span class=n>acc</span> <span class=o>=</span> <span class=n>_mm256_fmadd_ps</span><span class=p>(</span><span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>,</span> <span class=n>acc</span><span class=p>);</span>  <span class=c1>// Update the only accumulator  
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p><code>AvxVerticalFma2</code>Â version runs following code:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-C data-lang=C><span class=line><span class=cl><span class=k>for</span> <span class=p>(;</span> <span class=n>p1</span> <span class=o>&lt;</span> <span class=n>p1End</span><span class=p>;</span> <span class=n>p1</span> <span class=o>+=</span> <span class=mi>16</span><span class=p>,</span> <span class=n>p2</span> <span class=o>+=</span> <span class=mi>16</span><span class=p>)</span> <span class=p>{</span>  
</span></span><span class=line><span class=cl>    <span class=n>__m256</span> <span class=n>a</span> <span class=o>=</span> <span class=n>__mm256_loadu_ps_</span><span class=p>(</span><span class=n>p1</span><span class=p>);</span>  
</span></span><span class=line><span class=cl>    <span class=n>__m256</span> <span class=n>b</span> <span class=o>=</span> <span class=n>__mm256_loadu_ps_</span><span class=p>(</span><span class=n>p2</span><span class=p>);</span>  
</span></span><span class=line><span class=cl>    <span class=n>dot0</span> <span class=o>=</span> <span class=n>__mm256_fmadd_ps_</span><span class=p>(</span><span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>,</span> <span class=n>dot0</span><span class=p>);</span>  <span class=c1>// Update the first accumulator  
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>a</span> <span class=o>=</span> <span class=n>__mm256_loadu_ps_</span><span class=p>(</span><span class=n>p1</span> <span class=o>+</span> <span class=mi>8</span><span class=p>);</span>  
</span></span><span class=line><span class=cl>    <span class=n>b</span> <span class=o>=</span> <span class=n>__mm256_loadu_ps_</span><span class=p>(</span><span class=n>p2</span> <span class=o>+</span> <span class=mi>8</span><span class=p>);</span>  
</span></span><span class=line><span class=cl>    <span class=n>dot1</span> <span class=o>=</span> <span class=n>__mm256_fmadd_ps_</span><span class=p>(</span><span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>,</span> <span class=n>dot1</span><span class=p>);</span>  <span class=c1>// Update the second accumulator  
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><blockquote><p><code>_mm256_fmadd_ps</code>Â intrinsic computes (a*b)+c for arrays of eight float values, that instruction is part ofÂ 
<a href=https://en.wikipedia.org/wiki/FMA_instruction_set#FMA3_instruction_set rel=noopener>FMA3</a>Â instruction set. The reason whyÂ <code>AvxVerticalFma2</code>Â version is almost 2x fasterâ€”deeper pipelining hiding the latency.</p></blockquote><blockquote><p>When the processor submits an instruction, it needs values of the arguments. If some of them are not yet available, the processor waits for them to arrive. The tables onÂ 
<a href=https://www.agner.org/ rel=noopener>https://www.agner.org/</a>Â say on AMD Ryzen the latency of that FMA instruction is five cycles. This means once the processor started to execute that instruction, the result of the computation will only arrive five CPU cycles later. When the loop is running a single FMA instruction which needs the result computed by the previous loop iteration, that loop can only run one iteration in five CPU cycles.</p></blockquote><blockquote><p>With two accumulators that limit is the same, five cycles. However, the loop body now contains two FMA instructions that donâ€™t depend on each other. These two instructions run in parallel, and the code delivers twice the throughput on the desktop.</p></blockquote><a href=#optimizing-and-debugging-simd-code-hints><h1 id=optimizing-and-debugging-simd-code-hints><span class=hanchor arialabel=Anchor># </span>Optimizing and Debugging SIMD Code, Hints</h1></a><p>In the previous sections, we discussed how to vectorize code and handle conditional code. In this section, we will explore some common opportunities for improving the efficiency of SIMD code.</p><p>Instruction count is an important factor in program size and speed. As a general rule, shorter source code leads to smaller and faster programs. Advanced instructions, such as <code>_mm_blendv_ps</code>, can often replace a sequence of simpler instructions. It is useful to become familiar with the available instructions.</p><p>Floating point support in SSE and AVX is generally better than integer support. In some cases, converting temporary integers to floats can result in more efficient code, even if conversion back to integers is necessary. Floating point arithmetic is often simpler than integer intrinsics, some of which can be quite obscure, such as <code>_mm_mullo_epi32</code>.</p><p>Frequent use of <code>_mm_set_ps</code> to create constants in vectorized code can be costly since it takes four operands. To avoid this, consider caching the quadfloat outside loops so it can be used many times inside the loop without penalty. If scalars need to be expanded to quadfloats, consider caching the expanded version in the class.</p><p>Gather operations, which rely on scattered memory locations, can cause issues related to <code>_mm_set_ps</code>. For faster data retrieval from memory to quadfloat, it is best to have the data already stored as a quadfloat in 16 consecutive bytes of memory.</p><p>Data alignment is crucial when working with quadfloats since they must always be stored at a multiple of 16 in memory. In C++, variables created on the stack will automatically follow this rule, but variables allocated using new may not be aligned, leading to unexpected crashes. Checking whether the data being processed is properly aligned can help diagnose crashes.</p><p><img src="/notes/assets/img/O_Pasted image 20230216235536.png" width=auto></p><p>Support for SIMD is well-integrated in the Jetbrains Clion. It allows for effortless inspection of individual values in SIMD variables.</p><p>When using AVX/AVX2 instructions, ensure that the target hardware is compatible. If the code is not compatible, provide an alternative implementation for older hardware. Failing to do so may cause unexpected crashes or performance issues.</p><p>Focus vectorization efforts on bottlenecks only. In real-world situations, it is best to focus vectorization efforts on bottlenecks only, such as a large loop that updates variables.</p><p>Evade fancy SIMD libraries Vectorization is hard, and it feels unnatural to write <code>_mm_mul_ps(a,b)</code> when you meant to write <code>a * b</code>. Resist the urge to write your own operators; get used to the raw intrinsics. Anything more complex is bound to hide inefficiencies or even introduce them. Besides, some SIMD in your code makes it look like wizardry (which it is, in fact). If you must use something convenient, consider Agner Fogâ€™s vector library:
<a href=http://www.agner.org/optimize/#vectorclass rel=noopener>http://www.agner.org/optimize/#vectorclass</a> . Also read the rest of his site, the man is a guru of software optimization.</p><a href=#see-also><h1 id=see-also><span class=hanchor arialabel=Anchor># </span>See Also</h1></a><ul><li><a href=http://ftp.cvut.cz/kernel/people/geoff/cell/ps3-linux-docs/CellProgrammingTutorial/BasicsOfSIMDProgramming.html rel=noopener>http://ftp.cvut.cz/kernel/people/geoff/cell/ps3-linux-docs/CellProgrammingTutorial/BasicsOfSIMDProgramming.html</a></li><li><a href=https://www.agner.org/optimize/ rel=noopener>https://www.agner.org/optimize/</a></li><li><a href=https://software.intel.com/sites/landingpage/IntrinsicsGuide/ rel=noopener>https://software.intel.com/sites/landingpage/IntrinsicsGuide/</a></li><li><a href=https://envs.sh/5o rel=noopener>http://www.cs.uu.nl/docs/vakken/magr/2017-2018/files/SIMD%20Tutorial.pdf</a></li><li><a href=https://en.wikipedia.org/wiki/Advanced_Vector_Extensions rel=noopener>https://en.wikipedia.org/wiki/Advanced_Vector_Extensions</a></li><li><a href=https://stackoverflow.blog/2020/07/08/improving-performance-with-simd-intrinsics-in-three-use-cases/ rel=noopener>https://stackoverflow.blog/2020/07/08/improving-performance-with-simd-intrinsics-in-three-use-cases/</a></li></ul></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/notes/Misc/ data-ctx="notes/SIMD Intrinsics" data-src=/notes/Misc class=internal-link>Misc</a></li><li><a href=/notes/Resources/ data-ctx="notes/SIMD Intrinsics" data-src=/notes/Resources class=internal-link>Resources</a></li></ul></div><div><script async src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://xeome.github.io/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by xeome using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2023</p><ul><li><a href=https://xeome.github.io/>Home</a></li><li><a href=https://github.com/xeome>Github</a></li></ul></footer></div></div></body></html>